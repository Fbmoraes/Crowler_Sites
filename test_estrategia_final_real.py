"""
CONCLUS√ÉO FINAL: MatConcasa
============================

DESCOBERTA:
- Sitemap tem 21k URLs (categorias + subcategorias)
- Homepage TEM 81 produtos
- Categorias N√ÉO t√™m produtos listados (ou tem pagina√ß√£o)
- Produtos s√≥ est√£o em /produto/slug-123

ESTRAT√âGIA FINAL SIMPLES:
==========================
J√° que categorias n√£o listam produtos, a melhor estrat√©gia √©:

1. Buscar sitemap (1 request)
2. Detectar padr√£o na homepage (j√° temos: /produto/.*-\d+)
3. Filtrar sitemap pelo padr√£o
4. PROBLEMA: Sitemap n√£o tem /produto/ URLs!

SOLU√á√ÉO REAL:
=============
Como o sitemap N√ÉO tem produtos, s√≥ temos 2 op√ß√µes:

OP√á√ÉO A (SIMPLES): 
   - Extrair apenas da homepage (81 produtos)
   - R√°pido, mas limitado

OP√á√ÉO B (COMPLETA):
   - Crawl recursivo com limite de profundidade
   - Lento, mas pega tudo
   
OP√á√ÉO C (H√çBRIDA - RECOMENDADA):
   - Extrair homepage (81 produtos)
   - Navegar categorias principais buscando MAIS produtos
   - Se categoria n√£o tem, ignorar
   - Limite: 50 categorias √ó 1 request = 50 requests total
"""

import asyncio
import httpx
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
import time

async def estrategia_hibrida_real():
    """Estrat√©gia que REALMENTE funciona no MatConcasa"""
    
    print("="*70)
    print("üß™ ESTRAT√âGIA FINAL H√çBRIDA - MatConcasa")
    print("="*70)
    print("\nüìã PLANO:")
    print("   1. Homepage ‚Üí extrair produtos")
    print("   2. Sitemap ‚Üí pegar todas URLs")
    print("   3. Visitar URLs do sitemap buscando produtos")
    print("   4. Parar quando: encontrar N produtos OU visitar M p√°ginas\n")
    
    inicio = time.time()
    base_url = 'https://www.matconcasa.com.br/'
    produtos = set()
    paginas_visitadas = 0
    max_paginas = 30
    max_produtos = 500
    
    async with httpx.AsyncClient(timeout=15, follow_redirects=True) as client:
        # PASSO 1: Homepage
        print("üè† PASSO 1: Extraindo produtos da homepage...")
        r = await client.get(base_url)
        paginas_visitadas += 1
        soup = BeautifulSoup(r.text, 'html.parser')
        
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if '/produto/' in href:
                url = urljoin(base_url, href)
                produtos.add(url)
        
        print(f"   ‚úÖ {len(produtos)} produtos na homepage\n")
        
        # PASSO 2: Buscar sitemap
        print("üìÑ PASSO 2: Buscando URLs do sitemap...")
        r = await client.get(f'{base_url}sitemap.xml')
        paginas_visitadas += 1
        urls_sitemap = re.findall(r'<loc>(.*?)</loc>', r.text)
        
        # Filtra URLs candidatas (n√£o muito longas, n√£o produtos)
        urls_candidatas = []
        for url in urls_sitemap:
            path = urlparse(url).path
            segmentos = [s for s in path.split('/') if s]
            
            # Pega URLs de n√≠vel 1-3, que n√£o sejam produtos
            if 1 <= len(segmentos) <= 3 and '/produto/' not in url:
                # Ignora URLs muito longas (provavelmente produtos disfar√ßados)
                if len(path) < 100:
                    urls_candidatas.append(url)
        
        print(f"   ‚úÖ {len(urls_candidatas)} URLs candidatas\n")
        
        # PASSO 3: Navegar URLs candidatas
        print(f"üîç PASSO 3: Navegando at√© {max_paginas} p√°ginas...\n")
        
        for i, url in enumerate(urls_candidatas[:max_paginas], 1):
            if len(produtos) >= max_produtos:
                print(f"\n‚úÖ Limite de {max_produtos} produtos atingido!")
                break
            
            nome_pag = url.split('/')[-1] or 'home'
            print(f"   [{i:2d}] {nome_pag[:50]:<50}", end=' ')
            
            try:
                r = await client.get(url)
                paginas_visitadas += 1
                soup = BeautifulSoup(r.text, 'html.parser')
                
                novos = 0
                for link in soup.find_all('a', href=True):
                    href = link.get('href')
                    if '/produto/' in href:
                        produto_url = urljoin(url, href)
                        if produto_url not in produtos:
                            produtos.add(produto_url)
                            novos += 1
                
                if novos > 0:
                    print(f"‚úÖ +{novos:3d} ({len(produtos):4d} total)")
                else:
                    print(f"‚ö†Ô∏è   0")
                
                await asyncio.sleep(0.2)  # Delay
                
            except Exception as e:
                print(f"‚ùå Erro")
    
    tempo_total = time.time() - inicio
    
    # RESULTADOS
    print("\n" + "="*70)
    print("üìä RESULTADOS FINAIS")
    print("="*70)
    print(f"\n‚è±Ô∏è  Tempo: {tempo_total:.1f}s")
    print(f"üåê P√°ginas visitadas: {paginas_visitadas}")
    print(f"üì¶ Produtos encontrados: {len(produtos)}")
    print(f"\nüìà Performance:")
    print(f"   ‚Ä¢ {len(produtos)/tempo_total:.1f} produtos/segundo")
    print(f"   ‚Ä¢ {tempo_total/paginas_visitadas:.2f}s por p√°gina")
    print(f"   ‚Ä¢ {len(produtos)/paginas_visitadas:.1f} produtos por p√°gina")
    
    print(f"\nüì¶ Amostra (10 produtos):")
    for i, p in enumerate(list(produtos)[:10], 1):
        print(f"   {i:2d}. {p.split('/')[-1][:70]}")
    
    print("\n" + "="*70)
    print("üéØ AVALIA√á√ÉO DA ESTRAT√âGIA")
    print("="*70)
    
    if len(produtos) >= 400:
        print("\n‚úÖ EXCELENTE! Estrat√©gia vi√°vel")
        print(f"   ‚Ä¢ {len(produtos)} produtos em {tempo_total:.0f}s")
        print("   ‚Ä¢ Boa cobertura do cat√°logo")
        print("   ‚Ä¢ Tempo aceit√°vel")
    elif len(produtos) >= 200:
        print("\n‚ö†Ô∏è BOM, mas pode melhorar")
        print(f"   ‚Ä¢ {len(produtos)} produtos em {tempo_total:.0f}s")
        print("   ‚Ä¢ Cobertura parcial")
        print("   ‚Ä¢ Considere aumentar max_paginas")
    else:
        print("\n‚ùå INSUFICIENTE")
        print(f"   ‚Ä¢ Apenas {len(produtos)} produtos")
        print("   ‚Ä¢ Estrat√©gia n√£o funciona bem neste site")
    
    return len(produtos), tempo_total

asyncio.run(estrategia_hibrida_real())
