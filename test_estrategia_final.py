"""
TESTE COMPLETO: ESTRAT√âGIA H√çBRIDA OTIMIZADA
=============================================

ESTRAT√âGIA:
1. Homepage ‚Üí Detectar padr√£o (1 request)
2. Buscar categorias PRINCIPAIS n√≠vel 1 (sitemap ou homepage)
3. Navegar cada categoria COM LIMITE (max 100 produtos/categoria)
4. Total: 20-30 requests, ~30-60 segundos

TESTE: MatConcasa
"""

import asyncio
import httpx
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
from typing import Set, List, Optional
import time

class CrawlerHibrido:
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.netloc = urlparse(base_url).netloc
        self.padrao_produto: Optional[re.Pattern] = None
        self.produtos: Set[str] = set()
        self.stats = {
            'tempo_inicio': time.time(),
            'requests': 0,
            'categorias_navegadas': 0,
            'produtos_encontrados': 0
        }
    
    async def fazer_request(self, url: str) -> str:
        """Faz request e conta estat√≠sticas"""
        self.stats['requests'] += 1
        async with httpx.AsyncClient(timeout=15, follow_redirects=True) as client:
            r = await client.get(url)
            return r.text
    
    async def passo1_detectar_padrao(self):
        """PASSO 1: Detecta padr√£o na homepage"""
        print("üîç PASSO 1: Detectando padr√£o de produtos")
        
        html = await self.fazer_request(self.base_url)
        soup = BeautifulSoup(html, 'html.parser')
        
        produtos_amostra = []
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if '/produto/' in href:
                url = urljoin(self.base_url, href)
                produtos_amostra.append(url)
                self.produtos.add(url)
        
        produtos_amostra = list(set(produtos_amostra))
        self.stats['produtos_encontrados'] = len(self.produtos)
        
        print(f"   ‚úÖ Homepage: {len(produtos_amostra)} produtos")
        
        # Detecta padr√£o
        if len(produtos_amostra) >= 5:
            padrao = re.compile(r'/produto/[^/]+-\d+/?$')
            matches = sum(1 for p in produtos_amostra[:20] if padrao.search(p))
            
            if matches / min(len(produtos_amostra), 20) >= 0.7:
                self.padrao_produto = padrao
                print(f"   ‚úÖ Padr√£o detectado: /produto/.*-\\d+")
                return True
        
        print("   ‚ö†Ô∏è Padr√£o n√£o detectado, usando heur√≠stica")
        return False
    
    async def passo2_buscar_categorias(self) -> List[str]:
        """PASSO 2: Busca categorias principais (n√≠vel 1)"""
        print("\nüìÇ PASSO 2: Buscando categorias principais")
        
        categorias = set()
        
        # Tenta sitemap primeiro
        try:
            sitemap_url = f"{urlparse(self.base_url).scheme}://{self.netloc}/sitemap.xml"
            html = await self.fazer_request(sitemap_url)
            urls_sitemap = re.findall(r'<loc>(.*?)</loc>', html)
            
            print(f"   üìÑ Sitemap: {len(urls_sitemap)} URLs")
            
            # Filtra categorias n√≠vel 1 (apenas 1 segmento ap√≥s dom√≠nio)
            for url in urls_sitemap:
                path = urlparse(url).path
                segmentos = [s for s in path.split('/') if s]
                
                # N√≠vel 1: /categoria
                if len(segmentos) == 1 and '/produto/' not in url:
                    categorias.add(url)
            
            print(f"   ‚úÖ {len(categorias)} categorias n√≠vel 1 encontradas")
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è Sitemap n√£o dispon√≠vel: {e}")
        
        # Fallback: homepage
        if len(categorias) < 5:
            print("   üè† Buscando categorias na homepage...")
            html = await self.fazer_request(self.base_url)
            soup = BeautifulSoup(html, 'html.parser')
            
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                url = urljoin(self.base_url, href)
                
                if urlparse(url).netloc != self.netloc:
                    continue
                
                path = urlparse(url).path
                segmentos = [s for s in path.split('/') if s]
                
                if len(segmentos) == 1:
                    categorias.add(url)
            
            print(f"   ‚úÖ {len(categorias)} categorias da homepage")
        
        return list(categorias)[:20]  # Limita a 20 categorias
    
    async def passo3_extrair_produtos_categoria(self, url_categoria: str, max_produtos: int = 100):
        """PASSO 3: Extrai produtos de UMA categoria"""
        try:
            html = await self.fazer_request(url_categoria)
            soup = BeautifulSoup(html, 'html.parser')
            
            produtos_novos = 0
            for link in soup.find_all('a', href=True):
                href = link.get('href')
                url = urljoin(url_categoria, href)
                
                if urlparse(url).netloc != self.netloc:
                    continue
                
                # Aplica padr√£o se dispon√≠vel
                if self.padrao_produto:
                    if self.padrao_produto.search(url) and url not in self.produtos:
                        self.produtos.add(url)
                        produtos_novos += 1
                else:
                    # Heur√≠stica: /produto/ e n√≠vel 3+
                    if '/produto/' in url and url not in self.produtos:
                        niveis = len([s for s in urlparse(url).path.split('/') if s])
                        if niveis >= 2:
                            self.produtos.add(url)
                            produtos_novos += 1
                
                if produtos_novos >= max_produtos:
                    break
            
            return produtos_novos
        
        except Exception as e:
            print(f"      ‚ùå Erro: {e}")
            return 0
    
    async def passo4_navegar_categorias(self, categorias: List[str], max_por_categoria: int = 100):
        """PASSO 4: Navega todas as categorias"""
        print(f"\nüõí PASSO 3: Navegando {len(categorias)} categorias")
        print(f"   Limite: {max_por_categoria} produtos por categoria\n")
        
        for i, cat_url in enumerate(categorias, 1):
            nome_cat = cat_url.split('/')[-1] or 'home'
            
            print(f"   [{i:2d}/{len(categorias)}] {nome_cat[:40]:<40}", end=' ')
            
            produtos_antes = len(self.produtos)
            novos = await self.passo3_extrair_produtos_categoria(cat_url, max_por_categoria)
            
            if novos > 0:
                print(f"‚úÖ +{novos:3d} produtos")
            else:
                print(f"‚ö†Ô∏è  0 produtos")
            
            self.stats['categorias_navegadas'] += 1
            self.stats['produtos_encontrados'] = len(self.produtos)
            
            # Delay para n√£o sobrecarregar
            await asyncio.sleep(0.3)
    
    async def executar(self):
        """Executa estrat√©gia completa"""
        print("="*70)
        print("üß™ TESTE: ESTRAT√âGIA H√çBRIDA OTIMIZADA")
        print("="*70)
        print(f"\nüåê Site: {self.base_url}\n")
        
        # Passo 1: Detectar padr√£o
        await self.passo1_detectar_padrao()
        
        # Passo 2: Buscar categorias
        categorias = await self.passo2_buscar_categorias()
        
        if not categorias:
            print("\n‚ùå Nenhuma categoria encontrada!")
            return
        
        # Passo 3: Navegar categorias
        await self.passo4_navegar_categorias(categorias, max_por_categoria=100)
        
        # Estat√≠sticas finais
        tempo_total = time.time() - self.stats['tempo_inicio']
        
        print("\n" + "="*70)
        print("üìä RESULTADOS FINAIS")
        print("="*70)
        print(f"\n‚è±Ô∏è  Tempo total: {tempo_total:.1f}s")
        print(f"üåê Requests HTTP: {self.stats['requests']}")
        print(f"üìÇ Categorias navegadas: {self.stats['categorias_navegadas']}")
        print(f"üì¶ Produtos encontrados: {len(self.produtos)}")
        print(f"\nüìà Performance:")
        print(f"   ‚Ä¢ {len(self.produtos)/tempo_total:.1f} produtos/segundo")
        print(f"   ‚Ä¢ {tempo_total/self.stats['requests']:.2f}s por request")
        print(f"   ‚Ä¢ {len(self.produtos)/self.stats['requests']:.1f} produtos por request")
        
        # Amostra de produtos
        print(f"\nüì¶ Amostra de 10 produtos:")
        for i, p in enumerate(list(self.produtos)[:10], 1):
            nome = p.split('/')[-1][:60]
            print(f"   {i:2d}. {nome}")
        
        return len(self.produtos), tempo_total

async def teste_comparativo():
    """Compara com a expectativa"""
    print("\n" + "="*70)
    print("üéØ AN√ÅLISE E COMPARA√á√ÉO")
    print("="*70)
    
    crawler = CrawlerHibrido('https://www.matconcasa.com.br/')
    total_produtos, tempo_total = await crawler.executar()
    
    print("\n" + "="*70)
    print("üìä COMPARA√á√ÉO COM OUTRAS ESTRAT√âGIAS")
    print("="*70)
    
    print("\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê")
    print("‚îÇ Estrat√©gia             ‚îÇ Produtos ‚îÇ Requests ‚îÇ Tempo    ‚îÇ Cobertura ‚îÇ")
    print("‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§")
    print(f"‚îÇ Sitemap V5 (validar)   ‚îÇ    0     ‚îÇ  21,000  ‚îÇ  71 min  ‚îÇ    0%     ‚îÇ")
    print(f"‚îÇ Crawl Recursivo        ‚îÇ   ~500   ‚îÇ  100-500 ‚îÇ  5-10min ‚îÇ   100%    ‚îÇ")
    print(f"‚îÇ H√≠brida Otimizada      ‚îÇ {total_produtos:>6}   ‚îÇ {crawler.stats['requests']:>7}  ‚îÇ {tempo_total:>6.0f}s  ‚îÇ   ~80%    ‚îÇ")
    print("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò")
    
    print("\n‚úÖ PONTOS FORTES:")
    print("   1. R√°pida: ~1 minuto vs 71 minutos do V5")
    print(f"   2. Eficiente: {crawler.stats['requests']} requests vs 21k do V5")
    print(f"   3. Boa cobertura: {total_produtos} produtos")
    print("   4. Previs√≠vel: tempo proporcional ao n√∫mero de categorias")
    print("   5. Seguro: delay entre requests evita bloqueio")
    
    print("\n‚ö†Ô∏è LIMITA√á√ïES:")
    print("   1. N√£o pega 100% (s√≥ categorias n√≠vel 1)")
    print("   2. Depende da estrutura do site (categorias principais)")
    print("   3. Limite por categoria pode deixar produtos de fora")
    
    print("\nüí° OTIMIZA√á√ïES POSS√çVEIS:")
    print("   1. Navegar tamb√©m categorias n√≠vel 2 (mais requests)")
    print("   2. Aumentar limite por categoria (100 ‚Üí 200)")
    print("   3. Paralelizar navega√ß√£o (async workers)")
    print("   4. Detectar pagina√ß√£o e navegar p√°ginas seguintes")
    
    print("\nüéØ RECOMENDA√á√ÉO FINAL:")
    if total_produtos >= 500 and tempo_total <= 120:
        print("   ‚úÖ APROVADO! Estrat√©gia vi√°vel e eficiente")
        print(f"   Conseguiu {total_produtos} produtos em {tempo_total:.0f}s")
        print("   Pronto para implementar no V8!")
    elif total_produtos >= 200:
        print("   ‚ö†Ô∏è APROVADO COM RESSALVAS")
        print(f"   Encontrou {total_produtos} produtos, mas pode n√£o ser 100%")
        print("   Considere adicionar n√≠vel 2 de categorias")
    else:
        print("   ‚ùå N√ÉO APROVADO")
        print(f"   Apenas {total_produtos} produtos - cobertura insuficiente")
        print("   Precisa revisar estrat√©gia")

asyncio.run(teste_comparativo())
