"""
EXTRA√á√ÉO HYPER-OTIMIZADA - Todas as t√©cnicas de performance
Meta: Reduzir de 1.35s ‚Üí <0.3s por produto

Otimiza√ß√µes aplicadas:
1. Resource blocking agressivo (CSS, fonts, analytics, imagens grandes)
2. Early-abort quando dados aparecem (n√£o espera load completo)
3. Concorr√™ncia m√°xima (50-80 p√°ginas paralelas)
4. Timeout agressivo (3s m√°ximo por p√°gina)
5. Sem retry (fail fast)
6. Cache de DNS e conex√µes
"""

import asyncio
import json
import time
from datetime import datetime, timedelta
from crawlee.crawlers import PlaywrightCrawler, PlaywrightCrawlingContext
from crawlee import ConcurrencySettings

# ============================================================================
# CONFIGURA√á√ïES ULTRA AGRESSIVAS
# ============================================================================
MAX_CONCURRENCY = 60           # 60 p√°ginas paralelas
NAVIGATION_TIMEOUT = 5000      # 5s m√°ximo para navega√ß√£o
SELECTOR_TIMEOUT = 3000        # 3s m√°ximo para achar h1
EARLY_ABORT_WAIT = 100         # 100ms ap√≥s h1 aparecer
MAX_RETRIES = 0                # Sem retry - fail fast

# Resources para BLOQUEAR (economiza ~60% do tempo)
BLOCKED_RESOURCE_TYPES = [
    'stylesheet',     # CSS
    'font',           # Fontes
    'image',          # Imagens (carregaremos s√≥ as necess√°rias depois)
    'media',          # V√≠deos
]

BLOCKED_URL_PATTERNS = [
    '*google-analytics*',
    '*googletagmanager*',
    '*facebook*',
    '*hotjar*',
    '*clarity*',
    '*doubleclick*',
    '*.woff*',
    '*.ttf*',
    '*.otf*',
    '*.mp4*',
    '*.webm*',
]

# Estat√≠sticas
stats = {
    'inicio': None,
    'fim': None,
    'produtos': [],
    'erros': [],
    'tempos': [],
}

# ============================================================================
# FUN√á√ÉO DE EXTRA√á√ÉO COM EARLY ABORT
# ============================================================================
async def extrair_produto_hyper_fast(context: PlaywrightCrawlingContext) -> None:
    """Extra√ß√£o ultra-r√°pida com early abort"""
    page = context.page
    url = context.request.url
    tempo_inicio = time.time()
    
    try:
        # ========================================
        # BLOQUEIO AGRESSIVO DE RECURSOS
        # ========================================
        async def block_resources(route):
            """Bloqueia recursos desnecess√°rios"""
            request = route.request
            
            # Bloquear por tipo
            if request.resource_type in BLOCKED_RESOURCE_TYPES:
                await route.abort()
                return
            
            # Bloquear por URL pattern
            for pattern in BLOCKED_URL_PATTERNS:
                if pattern.replace('*', '') in request.url:
                    await route.abort()
                    return
            
            # Continuar com o resto
            await route.continue_()
        
        # Ativar bloqueio
        await page.route('**/*', block_resources)
        
        # ========================================
        # NAVEGA√á√ÉO COM EARLY ABORT
        # ========================================
        # Iniciar navega√ß√£o sem esperar completar
        navigation_promise = page.goto(
            url,
            wait_until='domcontentloaded',  # N√£o espera 'load', s√≥ DOMContentLoaded
            timeout=NAVIGATION_TIMEOUT
        )
        
        # Esperar h1 aparecer (dados j√° carregados)
        try:
            await page.wait_for_selector('h1', timeout=SELECTOR_TIMEOUT, state='attached')
            # Dar s√≥ 100ms para React hidratar
            await page.wait_for_timeout(EARLY_ABORT_WAIT)
        except Exception as e:
            # Se h1 n√£o aparecer, tentar completar navega√ß√£o
            await navigation_promise
            raise Exception(f"h1 n√£o encontrado: {e}")
        
        # ========================================
        # EXTRA√á√ÉO ULTRA-R√ÅPIDA (1 EVALUATE)
        # ========================================
        resultado = await page.evaluate('''() => {
            // NOME - primeira h1 v√°lida
            const h1s = Array.from(document.querySelectorAll('h1'));
            const nome = h1s.find(h => {
                const text = h.innerText.trim();
                return text.length > 20 && 
                       !text.includes('Vendido') && 
                       !text.includes('Parceria') &&
                       /\\d/.test(text);
            })?.innerText.trim() || '';
            
            // PRE√áOS - regex no HTML inteiro
            const html = document.body.innerHTML;
            const precoMatch = html.match(/de\\s+R\\$\\s*([\\d.,]+).*?por.*?R\\$\\s*([\\d.,]+)/is) ||
                              html.match(/R\\$\\s*([\\d.,]+).*?R\\$\\s*([\\d.,]+)/s);
            
            let preco = '';
            let preco_original = '';
            
            if (precoMatch) {
                if (precoMatch[2]) {
                    preco_original = precoMatch[1].replace(/\\./g, '').replace(',', '.');
                    preco = precoMatch[2].replace(/\\./g, '').replace(',', '.');
                } else {
                    preco = precoMatch[1].replace(/\\./g, '').replace(',', '.');
                }
            } else {
                // Fallback: pegar primeiro pre√ßo
                const simplePriceMatch = html.match(/R\\$\\s*([\\d.,]+)/);
                if (simplePriceMatch) {
                    preco = simplePriceMatch[1].replace(/\\./g, '').replace(',', '.');
                }
            }
            
            // IMAGENS - s√≥ product images, n√£o carregamos (s√≥ pegamos URLs)
            const imgs = Array.from(document.querySelectorAll('img'))
                .map(img => img.src || img.getAttribute('data-src'))
                .filter(src => src && 
                              src.includes('matconcasa') && 
                              !src.includes('logo') &&
                              !src.includes('banner'));
            
            const imagens = [...new Set(imgs)].slice(0, 5);
            
            // DISPONIBILIDADE
            const bodyText = document.body.innerText.toLowerCase();
            const disponivel = !bodyText.includes('indispon√≠vel') && 
                             !bodyText.includes('esgotado');
            
            return {
                nome,
                preco,
                preco_original,
                imagens,
                disponivel
            };
        }''')
        
        # ========================================
        # VALIDA√á√ÉO E ESTAT√çSTICAS
        # ========================================
        tempo_total = time.time() - tempo_inicio
        stats['tempos'].append(tempo_total)
        
        if resultado['nome'] and resultado['preco']:
            stats['produtos'].append({
                **resultado,
                'url': url,
                'tempo': round(tempo_total, 3)
            })
            
            # Progress compacto
            count = len(stats['produtos'])
            avg_time = sum(stats['tempos']) / len(stats['tempos'])
            print(f"‚úÖ [{count:3d}] {resultado['nome'][:50]:50s} R$ {resultado['preco']:>8s} ({tempo_total:.2f}s | avg: {avg_time:.3f}s)")
        else:
            erro = f"Dados incompletos: nome={bool(resultado['nome'])} preco={bool(resultado['preco'])}"
            stats['erros'].append({'url': url, 'erro': erro, 'tempo': tempo_total})
            print(f"‚ùå [{len(stats['erros']):3d}] {url[:80]} - {erro}")
            
    except Exception as e:
        tempo_total = time.time() - tempo_inicio
        stats['erros'].append({'url': url, 'erro': str(e), 'tempo': tempo_total})
        stats['tempos'].append(tempo_total)
        print(f"‚ùå ERRO {url[:60]}: {str(e)[:50]}")

# ============================================================================
# MAIN - EXECUTAR CRAWLING
# ============================================================================
async def main():
    """Executar crawler hyper-otimizado"""
    
    print("\n" + "="*80)
    print("üöÄ EXTRA√á√ÉO HYPER-OTIMIZADA - Performance M√°xima")
    print("="*80)
    print(f"‚öôÔ∏è  Configura√ß√µes:")
    print(f"   ‚Ä¢ Concorr√™ncia: {MAX_CONCURRENCY} p√°ginas paralelas")
    print(f"   ‚Ä¢ Timeout navega√ß√£o: {NAVIGATION_TIMEOUT}ms")
    print(f"   ‚Ä¢ Timeout seletor: {SELECTOR_TIMEOUT}ms")
    print(f"   ‚Ä¢ Early abort: {EARLY_ABORT_WAIT}ms ap√≥s h1")
    print(f"   ‚Ä¢ Resource blocking: {len(BLOCKED_RESOURCE_TYPES)} tipos + {len(BLOCKED_URL_PATTERNS)} patterns")
    print(f"   ‚Ä¢ Retries: {MAX_RETRIES}")
    print("="*80 + "\n")
    
    # Ler URLs
    with open('urls_matcon_100.txt', 'r', encoding='utf-8') as f:
        urls = [line.strip() for line in f if line.strip()]
    
    print(f"üì¶ Total de URLs: {len(urls)}\n")
    
    stats['inicio'] = time.time()
    
    # Crawler com configura√ß√µes ultra-agressivas
    crawler = PlaywrightCrawler(
        request_handler=extrair_produto_hyper_fast,
        
        # Concorr√™ncia m√°xima
        concurrency_settings=ConcurrencySettings(
            max_concurrency=MAX_CONCURRENCY,
            min_concurrency=10,
            desired_concurrency=MAX_CONCURRENCY,
        ),
        
        # Performance
        max_requests_per_crawl=len(urls),
        max_request_retries=MAX_RETRIES,
        max_crawl_depth=0,
        request_handler_timeout=timedelta(seconds=10),  # 10s timeout como timedelta
        
        # Browser otimizado
        headless=True,
        browser_type='chromium',
    )
    
    # Executar
    await crawler.run(urls)
    
    stats['fim'] = time.time()
    
    # ========================================
    # RELAT√ìRIO FINAL
    # ========================================
    tempo_total = stats['fim'] - stats['inicio']
    total_items = len(stats['produtos']) + len(stats['erros'])
    tempo_medio = sum(stats['tempos']) / len(stats['tempos']) if stats['tempos'] else 0
    
    print("\n" + "="*80)
    print("üìä RELAT√ìRIO HYPER-OTIMIZADO")
    print("="*80)
    print(f"\n‚è±Ô∏è  Tempo total: {tempo_total:.2f}s ({tempo_total/60:.2f} minutos)")
    print(f"‚ö° Velocidade M√âDIA: {tempo_medio:.3f}s por produto")
    print(f"‚ö° Velocidade REAL: {tempo_total/total_items:.3f}s por produto (incluindo overhead)")
    
    print(f"\n‚úÖ Sucesso: {len(stats['produtos'])}/{total_items} ({len(stats['produtos'])/total_items*100:.1f}%)")
    print(f"‚ùå Erros: {len(stats['erros'])}/{total_items} ({len(stats['erros'])/total_items*100:.1f}%)")
    
    # Qualidade dos dados
    if stats['produtos']:
        with_nome = sum(1 for p in stats['produtos'] if p['nome'])
        with_preco = sum(1 for p in stats['produtos'] if p['preco'])
        with_preco_orig = sum(1 for p in stats['produtos'] if p['preco_original'])
        with_imgs = sum(1 for p in stats['produtos'] if p['imagens'])
        
        print(f"\nüìà Qualidade dos Dados:")
        print(f"   ‚Ä¢ Nome: {with_nome}/{len(stats['produtos'])} ({with_nome/len(stats['produtos'])*100:.1f}%)")
        print(f"   ‚Ä¢ Pre√ßo: {with_preco}/{len(stats['produtos'])} ({with_preco/len(stats['produtos'])*100:.1f}%)")
        print(f"   ‚Ä¢ Pre√ßo original: {with_preco_orig}/{len(stats['produtos'])} ({with_preco_orig/len(stats['produtos'])*100:.1f}%)")
        print(f"   ‚Ä¢ Imagens: {with_imgs}/{len(stats['produtos'])} ({with_imgs/len(stats['produtos'])*100:.1f}%)")
    
    # Estimativa para 800 produtos
    if tempo_medio > 0:
        est_800 = tempo_medio * 800
        print(f"\nüéØ Estimativa para 800 produtos: {est_800:.1f}s ({est_800/60:.2f} minutos)")
        print(f"   Meta: 120s (2 minutos)")
        print(f"   Diferen√ßa: {est_800 - 120:.1f}s ({(est_800/120):.1f}x mais lento)")
    
    # Salvar JSON
    output_file = 'resultados_hyper_optimized.json'
    output = {
        'metadata': {
            'data_extracao': datetime.now().isoformat(),
            'tempo_total_segundos': round(tempo_total, 2),
            'tempo_medio_por_produto': round(tempo_medio, 3),
            'total_produtos': len(stats['produtos']),
            'total_erros': len(stats['erros']),
            'taxa_sucesso': round(len(stats['produtos'])/total_items*100, 1) if total_items > 0 else 0,
            'configuracao': {
                'max_concurrency': MAX_CONCURRENCY,
                'navigation_timeout': NAVIGATION_TIMEOUT,
                'selector_timeout': SELECTOR_TIMEOUT,
                'early_abort_wait': EARLY_ABORT_WAIT,
                'max_retries': MAX_RETRIES,
                'blocked_resources': BLOCKED_RESOURCE_TYPES,
            }
        },
        'produtos': stats['produtos'],
        'erros': stats['erros']
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ Resultados salvos: {output_file}")
    
    # Primeiro 3 produtos
    if stats['produtos']:
        print(f"\nüì¶ Primeiros 3 produtos extra√≠dos:")
        print("-"*80)
        for i, p in enumerate(stats['produtos'][:3], 1):
            print(f"\n{i}. {p['nome']}")
            print(f"   Pre√ßo: R$ {p['preco']}")
            if p['preco_original']:
                print(f"   De: R$ {p['preco_original']}")
            print(f"   Imagens: {len(p['imagens'])}")
            print(f"   Tempo: {p['tempo']}s")
    
    print("\n" + "="*80)
    if len(stats['produtos'])/total_items >= 0.95:
        print("üéâ EXCELENTE! Taxa de sucesso acima de 95%")
    elif len(stats['produtos'])/total_items >= 0.80:
        print("‚úÖ BOM! Taxa de sucesso acima de 80%")
    else:
        print("‚ö†Ô∏è  ATEN√á√ÉO! Taxa de sucesso abaixo de 80% - pode precisar ajustes")
    print("="*80 + "\n")

if __name__ == '__main__':
    asyncio.run(main())
