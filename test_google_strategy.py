"""
INVESTIGA√á√ÉO: Como o Google indexa MatConcasa?
===============================================

HIP√ìTESES:
1. Sitemap XML separado para produtos
2. RSS Feed de produtos
3. API p√∫blica
4. Links internos (descoberta por crawling)
5. Schema.org / JSON-LD
"""

import asyncio
import httpx
import re
from bs4 import BeautifulSoup

async def investigar_google():
    print("="*70)
    print("üîç INVESTIGANDO: Como Google indexa MatConcasa?")
    print("="*70)
    print()
    
    base_url = 'https://www.matconcasa.com.br'
    
    async with httpx.AsyncClient(timeout=15, follow_redirects=True) as client:
        
        # 1. ROBOTS.TXT
        print("ü§ñ PASSO 1: Verificando robots.txt")
        print("-" * 70)
        try:
            r = await client.get(f'{base_url}/robots.txt')
            print(r.text[:1000])
            
            # Procura sitemaps mencionados
            sitemaps = re.findall(r'Sitemap: (.*)', r.text)
            print(f"\n‚úÖ Sitemaps encontrados em robots.txt:")
            for sm in sitemaps:
                print(f"   ‚Ä¢ {sm}")
        except Exception as e:
            print(f"‚ùå Erro: {e}")
        
        print("\n")
        
        # 2. SITEMAP INDEX
        print("üìÑ PASSO 2: Verificando sitemap index")
        print("-" * 70)
        try:
            r = await client.get(f'{base_url}/sitemap.xml')
            
            # Procura refer√™ncias a outros sitemaps
            if 'sitemapindex' in r.text.lower():
                print("‚úÖ √â um SITEMAP INDEX (aponta para outros sitemaps)")
                
                outros_sitemaps = re.findall(r'<loc>(.*?\.xml)</loc>', r.text)
                print(f"\nüìã {len(outros_sitemaps)} sitemaps filhos:")
                for sm in outros_sitemaps[:10]:
                    print(f"   ‚Ä¢ {sm}")
                
                if len(outros_sitemaps) > 10:
                    print(f"   ... e mais {len(outros_sitemaps) - 10}")
                
                # Testa o primeiro sitemap filho
                if outros_sitemaps:
                    print(f"\nüß™ Testando primeiro sitemap: {outros_sitemaps[0]}")
                    r2 = await client.get(outros_sitemaps[0])
                    urls = re.findall(r'<loc>(.*?)</loc>', r2.text)
                    
                    # Verifica se tem produtos
                    produtos = [u for u in urls if '/produto/' in u]
                    print(f"   üì¶ {len(produtos)} produtos de {len(urls)} URLs")
                    
                    if produtos:
                        print(f"\n   ‚úÖ ENCONTROU PRODUTOS!")
                        print(f"   Exemplos:")
                        for p in produtos[:5]:
                            print(f"      ‚Ä¢ {p}")
            else:
                print("‚ö†Ô∏è √â um sitemap simples (n√£o √© index)")
                urls = re.findall(r'<loc>(.*?)</loc>', r.text)
                produtos = [u for u in urls if '/produto/' in u]
                print(f"   üì¶ {len(produtos)} produtos de {len(urls)} URLs")
        
        except Exception as e:
            print(f"‚ùå Erro: {e}")
        
        print("\n")
        
        # 3. LINKS INTERNOS
        print("üîó PASSO 3: Analisando links internos na homepage")
        print("-" * 70)
        try:
            r = await client.get(base_url)
            soup = BeautifulSoup(r.text, 'html.parser')
            
            # Conta todos os links
            todos_links = soup.find_all('a', href=True)
            print(f"üìä Total de links: {len(todos_links)}")
            
            # Categoriza
            links_produto = []
            links_categoria = []
            links_outros = []
            
            for link in todos_links:
                href = link.get('href', '')
                
                if '/produto/' in href:
                    links_produto.append(href)
                elif href.startswith('/') and len(href.split('/')) >= 2:
                    links_categoria.append(href)
                else:
                    links_outros.append(href)
            
            print(f"   üì¶ Links de produtos: {len(set(links_produto))}")
            print(f"   üìÇ Links de categorias: {len(set(links_categoria))}")
            print(f"   üìÑ Outros links: {len(set(links_outros))}")
            
            print(f"\n   üí° Google pode descobrir {len(set(links_produto))} produtos na homepage")
            
        except Exception as e:
            print(f"‚ùå Erro: {e}")
        
        print("\n")
        
        # 4. SCHEMA.ORG / JSON-LD
        print("üìã PASSO 4: Verificando Schema.org e marca√ß√£o estruturada")
        print("-" * 70)
        try:
            # Testa uma p√°gina de produto
            r = await client.get(base_url)
            soup = BeautifulSoup(r.text, 'html.parser')
            
            # Procura primeiro produto
            primeiro_produto = None
            for link in soup.find_all('a', href=True):
                if '/produto/' in link.get('href', ''):
                    from urllib.parse import urljoin
                    primeiro_produto = urljoin(base_url, link.get('href'))
                    break
            
            if primeiro_produto:
                print(f"üß™ Testando: {primeiro_produto.split('/')[-1][:50]}")
                r = await client.get(primeiro_produto)
                
                # Procura JSON-LD
                if 'application/ld+json' in r.text:
                    print("   ‚úÖ JSON-LD encontrado (Google adora isso!)")
                    
                    # Extrai
                    soup = BeautifulSoup(r.text, 'html.parser')
                    scripts = soup.find_all('script', type='application/ld+json')
                    
                    import json
                    for script in scripts:
                        try:
                            data = json.loads(script.string)
                            if '@type' in data:
                                print(f"   ‚Ä¢ Tipo: {data['@type']}")
                        except:
                            pass
                
                # Procura Open Graph
                og_tags = re.findall(r'<meta property="og:(.*?)" content="(.*?)"', r.text)
                if og_tags:
                    print(f"   ‚úÖ {len(og_tags)} tags Open Graph")
                
                # Procura microdados
                if 'itemtype' in r.text or 'itemscope' in r.text:
                    print("   ‚úÖ Microdados schema.org encontrados")
        
        except Exception as e:
            print(f"‚ùå Erro: {e}")
        
        print("\n")
        
        # 5. CONCLUS√ÉO
        print("="*70)
        print("üí° CONCLUS√ÉO: Como Google indexa MatConcasa")
        print("="*70)
        print("""
DESCOBERTAS:

1. SITEMAP: Verificar se existe sitemap INDEX com sitemap de produtos
   ‚Üí Se sim, Google usa esse sitemap espec√≠fico
   ‚Üí Se n√£o, Google crawla pela homepage

2. HOMEPAGE: Tem 81 produtos linkados diretamente
   ‚Üí Google descobre esses produtos na primeira visita

3. CRAWLING INTERNO: Google segue links internos
   ‚Üí Se categorias linkam para produtos, Google encontra
   ‚Üí MatConcasa pode ter links que BeautifulSoup n√£o v√™ (JS)

4. PAGINA√á√ÉO: Categorias podem ter pagina√ß√£o
   ‚Üí Google segue rel="next" ou links de p√°gina 2, 3, etc.

5. ESTRAT√âGIA DO GOOGLE:
   ‚úÖ Visita homepage ‚Üí Descobre 81 produtos
   ‚úÖ Segue links de categorias ‚Üí Descobre mais produtos
   ‚úÖ Segue pagina√ß√£o ‚Üí Descobre resto
   ‚úÖ Usa sitemap como backup/complemento

PARA N√ìS:
=========
Precisamos fazer o mesmo que o Google:
1. Extrair produtos da homepage
2. Seguir links de categorias
3. Detectar e seguir pagina√ß√£o
4. Verificar se existe sitemap de produtos separado
        """)

asyncio.run(investigar_google())
