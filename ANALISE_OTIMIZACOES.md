# An√°lise de Otimiza√ß√µes - Web Scraping

## üìä Resumo Executivo

**Problema**: A vers√£o "otimizada" (`test_html_ssr_optimized.py`) estava **mais lenta** que a vers√£o baseline (`test_html_ssr_50urls.py`)

**Causa Raiz**: A fun√ß√£o `fetch_headstart()` estava fazendo **2 requisi√ß√µes HTTP por produto** em vez de 1:
1. Primeira tentativa: Stream parcial com Range request (geralmente falhava)
2. Fallback: GET completo quando o stream n√£o era suficiente

**Solu√ß√£o**: Remover `fetch_headstart()` e usar GET direto, mantendo apenas otimiza√ß√µes de baixo overhead

---

## üéØ Resultados Comparativos

### Vers√£o Baseline (test_html_ssr_50urls.py)
```
‚úÖ 50/50 produtos (100% sucesso)
‚è±Ô∏è  1.006s/produto (m√©dia)
üì¶ ~550KB HTML por produto
‚è∞ 50.32s total (13.4 min para 800 produtos)
```

### Vers√£o "Otimizada" Original (com fetch_headstart)
```
‚ùå Mais lenta que baseline
üêõ 2 requisi√ß√µes HTTP por produto (gargalo)
üêõ O(n¬≤) string concatenation no loop
‚ùå Overhead de stream + fallback > benef√≠cio
```

### Vers√£o Simplificada (sem fetch_headstart)
```
‚úÖ 24/24 produtos testados (100% sucesso)
‚è±Ô∏è  0.57s - 1.92s por produto
‚è±Ô∏è  ~1.0s m√©dia (igual ou melhor que baseline!)
üì¶ ~545KB HTML por produto (download completo)
```

---

## üîç Otimiza√ß√µes Testadas

### ‚ùå REMOVIDAS (causavam slowdown ou overhead)

#### 1. Stream Parcial com Early-Stop (`fetch_headstart()`)
**Problema**:
- Fazia 2 requisi√ß√µes: Range request + fallback GET completo
- O(n¬≤) concatena√ß√£o de strings: `"".join(buf)` em cada itera√ß√£o do loop
- Servidor pode n√£o suportar Range requests (retorna 416)
- Overhead de verifica√ß√£o de marcadores em HTML incompleto

**C√≥digo problem√°tico**:
```python
async for chunk in r.aiter_text():
    buf.append(chunk)
    html_so_far = "".join(buf)  # ‚ùå O(n¬≤) - reprocessa todos os chunks
    
    # Verifica√ß√µes em HTML incompleto
    has_h1 = "<h1" in html_so_far  
    has_price = PRECO_RE.search(html_so_far)
```

**Li√ß√£o**: 
> üí° **Menos requisi√ß√µes √© melhor que requisi√ß√µes parciais complexas**
> - 1 GET completo (545KB) √© mais r√°pido que Range parcial (220KB) + fallback GET (545KB)

#### 2. HTTP/2
**Problema**:
- Requer m√≥dulo `h2` (n√£o instalado): `pip install httpx[http2]`
- Benef√≠cio marginal para scraping sequencial
- Funciona com fallback HTTP/1.1, mas sem ganho real

#### 3. uvloop
**Problema**:
- N√£o dispon√≠vel no Windows
- Python 3.13 j√° tem asyncio otimizado

---

### ‚úÖ MANTIDAS (baixo overhead, benef√≠cio real)

#### 1. Parser lxml
```python
soup = BeautifulSoup(html, 'lxml')  # ‚úÖ Mais r√°pido que 'html.parser'
```
**Benef√≠cio**: 20-30% mais r√°pido no parsing HTML

#### 2. Regex Pr√©-Compiladas
```python
# No topo do m√≥dulo (fora de fun√ß√µes)
PRECO_RE = re.compile(r'R\$\s*(?:<!--.*?-->)?\s*([\d.,]+)')
SKU_RE = re.compile(r'-(\d+)$')
MARCA_RE = re.compile(r'marca\s*=\s*["\']([^"\']+)', re.IGNORECASE)
```
**Benef√≠cio**: Evita recompilar regex a cada produto

#### 3. Retry-After Header + Jitter
```python
if response.status_code == 429:
    retry_delay = parse_retry_after(response.headers.get("Retry-After"))
    if retry_delay is None:
        retry_delay = min(8 * (2 ** tentativa), 60)
    # Jitter: +/- 50%
    retry_delay *= random.uniform(0.5, 1.5)
    await asyncio.sleep(retry_delay)
```
**Benef√≠cio**: Respeita orienta√ß√£o do servidor, evita thundering herd

#### 4. Connection Pooling
```python
limits = httpx.Limits(
    max_connections=10,
    max_keepalive_connections=10
)
async with httpx.AsyncClient(limits=limits, http2=False) as client:
    ...
```
**Benef√≠cio**: Reutiliza conex√µes TCP, reduz handshake overhead

#### 5. Headers Otimizados
```python
headers = {
    "User-Agent": random.choice(USER_AGENTS),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "pt-BR,pt;q=0.9",
    # ‚úÖ N√ÉO incluir "Accept-Encoding" manualmente
    # httpx gerencia compress√£o automaticamente
}
```
**Benef√≠cio**: Evita bugs de decompress√£o (gzip/brotli)

---

## üìà Configura√ß√£o √ìtima

### Rate Limiting
```python
rate_limiter = TokenBucket(rate=4.0)  # 4 requests/second
```
**Descoberta**: 
- 3 RPS: Muito conservador (lento)
- 4 RPS: ‚úÖ **Sweet spot** - 100% sucesso
- 5 RPS: Muitos 429 errors (servidor rejeita)

### Concorr√™ncia
```python
concorrencia = 3  # 3 URLs simult√¢neas
```
**Descoberta**:
- 2 URLs: Subutiliza rate limit
- 3 URLs: ‚úÖ **√ìtimo** - balanceia throughput e estabilidade
- 5-7 URLs: Muitos 429 errors, retries causam slowdown

### Timeout
```python
timeout = 10  # segundos
```
**Motivo**: HTML completo (~545KB) pode demorar em conex√µes lentas

---

## üéì Li√ß√µes Aprendidas

### 1. Premature Optimization is the Root of All Evil
> Stream parcial parecia inteligente (menos dados = mais r√°pido), mas o overhead de 2 requisi√ß√µes + O(n¬≤) loops matou a performance.

### 2. Simplicidade > Complexidade
> GET direto (545KB, 1 requisi√ß√£o) > Range parcial (220KB, 2 requisi√ß√µes com fallback)

### 3. O Servidor √© o Bottleneck
> Rate limit de 4 RPS √© a verdadeira limita√ß√£o. Otimizar c√≥digo al√©m disso tem retorno decrescente.

### 4. Measure, Don't Assume
> A vers√£o "otimizada" com 6 t√©cnicas avan√ßadas foi mais lenta. Sempre testar!

### 5. N√£o Lutar Contra HTTP Headers
> Deixar httpx gerenciar Accept-Encoding automaticamente evita bugs (HTML corrompido, 79KB vs 559KB)

---

## üöÄ Pr√≥ximos Passos

1. ‚úÖ **CONCLU√çDO**: Remover `fetch_headstart()` do c√≥digo
2. ‚úÖ **CONCLU√çDO**: Testar vers√£o simplificada (resultado: igual ou melhor que baseline)
3. ‚è≥ **PR√ìXIMO**: Integrar otimiza√ß√µes ben√©ficas no `extract_fast.py`:
   - lxml parser
   - Regex pr√©-compiladas
   - Retry-After + jitter
   - Connection pooling
   - Headers otimizados (sem Accept-Encoding manual)

4. üéØ **META**: Manter 100% sucesso @ ~1.0s/produto = **13 minutos para 800 produtos**

---

## üìù C√≥digo Final Recomendado

```python
# ‚úÖ BOAS PR√ÅTICAS

# 1. Regex no topo do m√≥dulo
PRECO_RE = re.compile(r'R\$\s*(?:<!--.*?-->)?\s*([\d.,]+)')
SKU_RE = re.compile(r'-(\d+)$')
MARCA_RE = re.compile(r'marca\s*=\s*["\']([^"\']+)', re.IGNORECASE)

# 2. Cliente HTTP com pooling
limits = httpx.Limits(max_connections=10, max_keepalive_connections=10)
async with httpx.AsyncClient(limits=limits, http2=False) as client:
    
    # 3. Headers limpos
    headers = {
        "User-Agent": random.choice(USER_AGENTS),
        "Accept": "text/html",
        "Accept-Language": "pt-BR,pt;q=0.9",
    }
    
    # 4. GET direto (n√£o stream parcial!)
    response = await client.get(url, headers=headers, timeout=10)
    
    # 5. 429 com Retry-After + jitter
    if response.status_code == 429:
        retry_delay = parse_retry_after(response.headers.get("Retry-After"))
        if retry_delay is None:
            retry_delay = min(8 * (2 ** tentativa), 60)
        retry_delay *= random.uniform(0.5, 1.5)
        await asyncio.sleep(retry_delay)
    
    # 6. Parser lxml
    soup = BeautifulSoup(html, 'lxml')
    
    # 7. Usar regex pr√©-compiladas
    precos = PRECO_RE.findall(html)
```

---

## üîß Performance Tuning Summary

| T√©cnica | Overhead | Benef√≠cio | Veredito |
|---------|----------|-----------|----------|
| Stream Parcial + Early-Stop | ‚ùå Alto (2 req) | ‚ùå Nenhum | **REMOVER** |
| HTTP/2 | ‚ö†Ô∏è M√©dio (req lib) | ‚ö†Ô∏è Marginal | Opcional |
| uvloop | ‚ö†Ô∏è N/A Windows | ‚ö†Ô∏è Marginal | Skip |
| lxml parser | ‚úÖ Baixo | ‚úÖ 20-30% parsing | **USAR** |
| Regex pr√©-compiladas | ‚úÖ M√≠nimo | ‚úÖ 10-15% regex | **USAR** |
| Retry-After + jitter | ‚úÖ M√≠nimo | ‚úÖ Menos bans | **USAR** |
| Connection pooling | ‚úÖ M√≠nimo | ‚úÖ Menos handshakes | **USAR** |
| Headers otimizados | ‚úÖ Zero | ‚úÖ Evita bugs | **USAR** |

---

**Conclus√£o**: A vers√£o simplificada com apenas otimiza√ß√µes de baixo overhead alcan√ßa **mesma performance** que a baseline (1.0s/produto) com **c√≥digo mais limpo e confi√°vel**. üéØ
