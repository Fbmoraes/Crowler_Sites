"""
DIAGNÃ“STICO: ESTRATÃ‰GIA DE CRAWLING RECURSIVO
==============================================

PROBLEMA IDENTIFICADO:
- MatConcasa sitemap: categorias SEM /produto/ no path
- Produtos: /produto/slug-123 (NÃƒO estÃ£o no sitemap)
- Produtos aparecem nas PÃGINAS de categoria

ESTRATÃ‰GIA PROPOSTA:
1. Detectar padrÃ£o de produto (ex: /produto/.*-\d+)
2. ComeÃ§ar pela homepage
3. Clicar em TODOS os links que NÃƒO sÃ£o produtos
4. Em cada pÃ¡gina visitada:
   - Se link Ã© produto â†’ GUARDAR
   - Se link NÃƒO Ã© produto â†’ VISITAR (se ainda nÃ£o visitou)
5. Parar quando nÃ£o houver mais links novos

Ã‰ como um SPIDER/CRAWLER tradicional!
"""

import asyncio
import httpx
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
from typing import Set, Dict, List, Optional

class CrawlerRecursivo:
    def __init__(self, base_url: str, padrao_produto: Optional[re.Pattern] = None):
        self.base_url = base_url
        self.netloc = urlparse(base_url).netloc
        self.padrao_produto = padrao_produto
        
        # Rastreamento
        self.produtos_encontrados: Set[str] = set()
        self.paginas_visitadas: Set[str] = set()
        self.paginas_para_visitar: Set[str] = {base_url}
        
        # EstatÃ­sticas
        self.stats = {
            'paginas_visitadas': 0,
            'produtos_encontrados': 0,
            'links_analisados': 0
        }
    
    def eh_produto(self, url: str) -> bool:
        """Verifica se URL Ã© de produto"""
        if self.padrao_produto:
            return bool(self.padrao_produto.search(url))
        
        # Fallback: heurÃ­stica genÃ©rica
        return '/produto/' in url
    
    def eh_link_valido(self, url: str) -> bool:
        """Verifica se link Ã© vÃ¡lido para crawling"""
        parsed = urlparse(url)
        
        # Mesma origem
        if parsed.netloc != self.netloc:
            return False
        
        # Ignora Ã¢ncoras, query strings complexas
        if '#' in url:
            return False
        
        # Ignora arquivos estÃ¡ticos
        extensoes_ignorar = ['.jpg', '.png', '.pdf', '.zip', '.css', '.js']
        if any(url.lower().endswith(ext) for ext in extensoes_ignorar):
            return False
        
        # Ignora pÃ¡ginas institucionais
        palavras_ignorar = ['login', 'cadastro', 'cart', 'checkout', 'conta', 'minha-conta']
        if any(palavra in url.lower() for palavra in palavras_ignorar):
            return False
        
        return True
    
    async def extrair_links(self, url: str) -> Dict[str, List[str]]:
        """Extrai links de uma pÃ¡gina, separando produtos e nÃ£o-produtos"""
        try:
            async with httpx.AsyncClient(timeout=15, follow_redirects=True) as client:
                r = await client.get(url)
                soup = BeautifulSoup(r.text, 'html.parser')
                
                links_produtos = []
                links_navegacao = []
                
                for link in soup.find_all('a', href=True):
                    href = link.get('href')
                    url_completa = urljoin(url, href)
                    
                    if not self.eh_link_valido(url_completa):
                        continue
                    
                    self.stats['links_analisados'] += 1
                    
                    if self.eh_produto(url_completa):
                        links_produtos.append(url_completa)
                    else:
                        links_navegacao.append(url_completa)
                
                return {
                    'produtos': list(set(links_produtos)),
                    'navegacao': list(set(links_navegacao))
                }
        
        except Exception as e:
            print(f"   âŒ Erro ao extrair links de {url}: {e}")
            return {'produtos': [], 'navegacao': []}
    
    async def crawl(self, max_paginas: int = 50, max_produtos: int = None):
        """Crawl recursivo do site"""
        print("ğŸ•·ï¸ INICIANDO CRAWLING RECURSIVO")
        print(f"   Base: {self.base_url}")
        print(f"   PadrÃ£o: {self.padrao_produto.pattern if self.padrao_produto else 'HeurÃ­stica'}")
        print(f"   Limites: {max_paginas} pÃ¡ginas, {max_produtos or 'âˆ'} produtos\n")
        
        while self.paginas_para_visitar and len(self.paginas_visitadas) < max_paginas:
            # Pega prÃ³xima pÃ¡gina
            url_atual = self.paginas_para_visitar.pop()
            
            # JÃ¡ visitou?
            if url_atual in self.paginas_visitadas:
                continue
            
            # Marca como visitada
            self.paginas_visitadas.add(url_atual)
            self.stats['paginas_visitadas'] += 1
            
            print(f"[{self.stats['paginas_visitadas']:2d}] Visitando: {url_atual.split('/')[-1] or 'home'}")
            
            # Extrai links
            links = await self.extrair_links(url_atual)
            
            # Adiciona produtos encontrados
            novos_produtos = [p for p in links['produtos'] if p not in self.produtos_encontrados]
            self.produtos_encontrados.update(novos_produtos)
            self.stats['produtos_encontrados'] = len(self.produtos_encontrados)
            
            if novos_produtos:
                print(f"      â†’ ğŸ“¦ {len(novos_produtos)} novos produtos (total: {self.stats['produtos_encontrados']})")
            
            # Adiciona pÃ¡ginas para visitar
            novos_links = [l for l in links['navegacao'] if l not in self.paginas_visitadas]
            self.paginas_para_visitar.update(novos_links)
            
            if novos_links:
                print(f"      â†’ ğŸ”— {len(novos_links)} novas pÃ¡ginas (fila: {len(self.paginas_para_visitar)})")
            
            # Atingiu limite de produtos?
            if max_produtos and self.stats['produtos_encontrados'] >= max_produtos:
                print(f"\nâœ… Limite de {max_produtos} produtos atingido!")
                break
            
            # Delay para nÃ£o sobrecarregar
            await asyncio.sleep(0.5)
        
        return list(self.produtos_encontrados)

async def diagnostico_crawling():
    """TESTA A ESTRATÃ‰GIA NO MATCONCASA"""
    print("="*70)
    print("ğŸ§ª DIAGNÃ“STICO: CRAWLING RECURSIVO - MatConcasa")
    print("="*70)
    print()
    
    base_url = 'https://www.matconcasa.com.br/'
    
    # PASSO 1: Detectar padrÃ£o na homepage
    print("ğŸ“‹ PASSO 1: Detectando padrÃ£o de produtos\n")
    
    async with httpx.AsyncClient(timeout=15) as client:
        r = await client.get(base_url)
        soup = BeautifulSoup(r.text, 'html.parser')
        
        produtos_amostra = []
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if '/produto/' in href:
                url = urljoin(base_url, href)
                produtos_amostra.append(url)
        
        produtos_amostra = list(set(produtos_amostra))[:10]
        
        print(f"   Encontrados {len(produtos_amostra)} produtos na homepage")
        print(f"\n   ğŸ“¦ Exemplos:")
        for p in produtos_amostra[:3]:
            print(f"      {p}")
    
    # Detecta padrÃ£o
    padrao = re.compile(r'/produto/[^/]+-\d+/?$')
    matches = sum(1 for p in produtos_amostra if padrao.search(p))
    print(f"\n   ğŸ” PadrÃ£o testado: /produto/[^/]+-\\d+/?$")
    print(f"   âœ… Match: {matches}/{len(produtos_amostra)} ({matches/len(produtos_amostra)*100:.0f}%)")
    
    # PASSO 2: Crawl recursivo
    print(f"\n{'='*70}")
    print("ğŸ“‹ PASSO 2: Crawling recursivo (limitado a 20 pÃ¡ginas)")
    print("="*70)
    print()
    
    crawler = CrawlerRecursivo(base_url, padrao)
    produtos = await crawler.crawl(max_paginas=20, max_produtos=200)
    
    # RESULTADOS
    print(f"\n{'='*70}")
    print("ğŸ“Š RESULTADOS DO CRAWLING")
    print("="*70)
    print(f"\nâœ… PÃ¡ginas visitadas: {crawler.stats['paginas_visitadas']}")
    print(f"âœ… Links analisados: {crawler.stats['links_analisados']}")
    print(f"âœ… Produtos encontrados: {len(produtos)}")
    
    print(f"\nğŸ“¦ Primeiros 10 produtos:")
    for i, p in enumerate(produtos[:10], 1):
        nome = p.split('/')[-1][:50]
        print(f"   {i:2d}. {nome}")
    
    # ANÃLISE DA ESTRATÃ‰GIA
    print(f"\n{'='*70}")
    print("ğŸ¯ ANÃLISE DA ESTRATÃ‰GIA")
    print("="*70)
    
    print("\nâœ… PONTOS FORTES:")
    print("   1. Encontra TODOS os produtos do site (nÃ£o depende de sitemap)")
    print("   2. Funciona mesmo se produtos nÃ£o estÃ£o no sitemap")
    print("   3. Descobre produtos em qualquer nÃ­vel de profundidade")
    print("   4. Evita visitar mesma pÃ¡gina 2x (eficiente)")
    print("   5. PadrÃ£o filtra produtos automaticamente")
    
    print("\nâš ï¸ PONTOS DE ATENÃ‡ÃƒO:")
    print("   1. LENTO: precisa visitar muitas pÃ¡ginas")
    print(f"      â†’ MatConcasa: {crawler.stats['paginas_visitadas']} pÃ¡ginas para {len(produtos)} produtos")
    print(f"      â†’ Ratio: {crawler.stats['paginas_visitadas']/len(produtos):.2f} pÃ¡ginas/produto")
    print("   2. Muitos requests HTTP (pode ser bloqueado)")
    print("   3. DifÃ­cil estimar tempo total (depende da estrutura)")
    print("   4. Pode entrar em loops se site tiver filtros infinitos")
    
    print("\nğŸ’¡ OTIMIZAÃ‡Ã•ES NECESSÃRIAS:")
    print("   1. Limitar profundidade (ex: max 3 nÃ­veis)")
    print("   2. Priorizar categorias principais")
    print("   3. Adicionar rate limiting (delay entre requests)")
    print("   4. Detectar e ignorar filtros/paginaÃ§Ã£o")
    print("   5. Paralelizar crawling (async workers)")
    
    print("\nğŸ¯ RECOMENDAÃ‡ÃƒO FINAL:")
    if len(produtos) >= 100:
        print("   âœ… ESTRATÃ‰GIA VIÃVEL!")
        print(f"   Encontrou {len(produtos)} produtos em {crawler.stats['paginas_visitadas']} pÃ¡ginas")
        print("   âš ï¸ MAS: Implementar com limites e otimizaÃ§Ãµes")
        print("\n   ğŸ“‹ IMPLEMENTAR:")
        print("      - Crawl recursivo com max_depth=3")
        print("      - Rate limiting (0.5-1s entre requests)")
        print("      - Priorizar URLs de categorias (nÃ­vel 1-2)")
        print("      - Ignorar filtros (price, sort, page)")
    else:
        print("   âš ï¸ ESTRATÃ‰GIA INEFICIENTE")
        print(f"   Apenas {len(produtos)} produtos em {crawler.stats['paginas_visitadas']} pÃ¡ginas")
    
    # COMPARAÃ‡ÃƒO COM ESTRATÃ‰GIA ANTERIOR
    print(f"\n{'='*70}")
    print("ğŸ“Š COMPARAÃ‡ÃƒO: Crawling Recursivo vs NavegaÃ§Ã£o Categorias")
    print("="*70)
    
    print("\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
    print("â”‚ Aspecto                 â”‚ Crawl Recursivo  â”‚ NavegaÃ§Ã£o Categorias â”‚")
    print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
    print("â”‚ Cobertura               â”‚ 100% (tudo)      â”‚ 80-90% (depende)     â”‚")
    print("â”‚ Velocidade              â”‚ Lento            â”‚ RÃ¡pido               â”‚")
    print("â”‚ Requests HTTP           â”‚ Muitos (50-200)  â”‚ Poucos (10-30)       â”‚")
    print("â”‚ Complexidade            â”‚ Alta             â”‚ MÃ©dia                â”‚")
    print("â”‚ Risco de bloqueio       â”‚ Alto             â”‚ Baixo                â”‚")
    print("â”‚ Previsibilidade         â”‚ DifÃ­cil          â”‚ FÃ¡cil                â”‚")
    print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
    
    print("\nğŸ’¡ ESTRATÃ‰GIA HÃBRIDA RECOMENDADA:")
    print("   1. Tentar sitemap primeiro")
    print("   2. Se sitemap vazio â†’ NavegaÃ§Ã£o por categorias (sitemap ou homepage)")
    print("   3. Se categorias falham â†’ Crawl recursivo LIMITADO")
    print("   4. Sempre com padrÃ£o detectado para filtrar")

asyncio.run(diagnostico_crawling())
