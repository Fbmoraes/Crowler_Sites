import asyncio
import httpx
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import re

async def testar_paginacao():
    """Verifica se categorias t√™m pagina√ß√£o"""
    print("="*70)
    print("üîç TESTANDO PAGINA√á√ÉO EM CATEGORIAS")
    print("="*70)
    print()
    
    # Testa uma categoria
    categorias_teste = [
        'https://www.matconcasa.com.br/ferramentas',
        'https://www.matconcasa.com.br/banheiro',
        'https://www.matconcasa.com.br/materiais-de-construcao'
    ]
    
    async with httpx.AsyncClient(timeout=15, follow_redirects=True) as client:
        
        for cat_url in categorias_teste:
            print(f"üìÇ Categoria: {cat_url.split('/')[-1]}")
            print("-" * 70)
            
            try:
                r = await client.get(cat_url)
                soup = BeautifulSoup(r.text, 'html.parser')
                html = r.text
                
                # 1. Conta produtos na p√°gina 1
                produtos = []
                for link in soup.find_all('a', href=True):
                    href = link.get('href')
                    if '/produto/' in href:
                        url = urljoin(cat_url, href)
                        produtos.append(url)
                
                produtos = list(set(produtos))
                print(f"   üì¶ Produtos na p√°gina 1: {len(produtos)}")
                
                # 2. Procura links de pagina√ß√£o
                paginacao_patterns = [
                    r'[?&]page=(\d+)',
                    r'[?&]p=(\d+)',
                    r'/page/(\d+)',
                    r'/p/(\d+)',
                    r'[?&]pagina=(\d+)'
                ]
                
                paginas_encontradas = set()
                for pattern in paginacao_patterns:
                    matches = re.findall(pattern, html)
                    paginas_encontradas.update(matches)
                
                if paginas_encontradas:
                    print(f"   üìÑ Pagina√ß√£o detectada: p√°ginas {sorted(paginas_encontradas)[:10]}")
                else:
                    print(f"   ‚ö†Ô∏è  Pagina√ß√£o n√£o detectada no HTML")
                
                # 3. Procura bot√µes/links com texto "pr√≥xima", "next", "2", etc
                links_proxima = []
                for link in soup.find_all('a', href=True):
                    texto = link.get_text(strip=True).lower()
                    if any(palavra in texto for palavra in ['pr√≥xim', 'next', 'seguinte', '2', '‚Üí', '>']):
                        if len(texto) < 20:  # N√£o √© texto longo
                            links_proxima.append({
                                'texto': texto,
                                'href': link.get('href')
                            })
                
                if links_proxima:
                    print(f"   üîó Links de navega√ß√£o encontrados:")
                    for lnk in links_proxima[:5]:
                        print(f"      ‚Ä¢ '{lnk['texto']}' ‚Üí {lnk['href']}")
                
                # 4. Verifica se h√° indica√ß√£o de total de produtos
                if 'produto' in html.lower() and 'resultado' in html.lower():
                    # Procura padr√µes como "120 produtos" ou "mostrando 1-20 de 120"
                    matches = re.findall(r'(\d+)\s*produtos?', html, re.IGNORECASE)
                    if matches:
                        total = max(int(m) for m in matches)
                        print(f"   üìä Total indicado: ~{total} produtos")
                
                # 5. Tenta acessar p√°gina 2 diretamente
                print(f"\n   üß™ Testando p√°gina 2...")
                tentativas_p2 = [
                    f"{cat_url}?page=2",
                    f"{cat_url}?p=2",
                    f"{cat_url}/page/2"
                ]
                
                for url_p2 in tentativas_p2:
                    try:
                        r2 = await client.get(url_p2)
                        if r2.status_code == 200:
                            soup2 = BeautifulSoup(r2.text, 'html.parser')
                            
                            produtos_p2 = []
                            for link in soup2.find_all('a', href=True):
                                href = link.get('href')
                                if '/produto/' in href:
                                    url = urljoin(url_p2, href)
                                    produtos_p2.append(url)
                            
                            produtos_p2 = list(set(produtos_p2))
                            
                            if len(produtos_p2) > 0:
                                print(f"      ‚úÖ {url_p2}")
                                print(f"         üì¶ {len(produtos_p2)} produtos na p√°gina 2")
                                
                                # Mostra exemplos
                                if produtos_p2:
                                    print(f"         Exemplos:")
                                    for p in produtos_p2[:3]:
                                        print(f"            ‚Ä¢ {p.split('/')[-1][:50]}")
                                
                                break
                    except:
                        pass
                
                print()
                
            except Exception as e:
                print(f"   ‚ùå Erro: {e}\n")
        
        print("="*70)
        print("üí° CONCLUS√ÉO")
        print("="*70)
        print("""
Se categorias t√™m pagina√ß√£o:
‚úÖ Google descobre produtos seguindo links "pr√≥xima p√°gina"
‚úÖ Precisamos implementar detec√ß√£o de pagina√ß√£o
‚úÖ Estrat√©gia: seguir at√© p√°gina N ou at√© n√£o haver mais produtos

Se categorias N√ÉO t√™m produtos:
‚ö†Ô∏è Produtos podem estar em subcategorias mais profundas
‚ö†Ô∏è Ou produtos s√£o adicionados via JavaScript/API
‚ö†Ô∏è Precisamos de estrat√©gia diferente
        """)

asyncio.run(testar_paginacao())
