import asyncio
import httpx
import re
from urllib.parse import urlparse

async def investigar_sitemap():
    """Investiga a estrutura real do sitemap"""
    print("üîç INVESTIGANDO ESTRUTURA DO SITEMAP\n")
    
    async with httpx.AsyncClient(timeout=10) as client:
        r = await client.get('https://www.matconcasa.com.br/sitemap.xml')
        urls = re.findall(r'<loc>(.*?)</loc>', r.text)
        
        print(f"üìÑ Total: {len(urls)} URLs\n")
        
        # Analisa por n√∫mero de segmentos
        por_niveis = {}
        exemplos_por_nivel = {}
        
        for url in urls:
            path = urlparse(url).path
            segmentos = [s for s in path.split('/') if s]
            nivel = len(segmentos)
            
            por_niveis[nivel] = por_niveis.get(nivel, 0) + 1
            
            # Guarda exemplos
            if nivel not in exemplos_por_nivel:
                exemplos_por_nivel[nivel] = []
            if len(exemplos_por_nivel[nivel]) < 5:
                exemplos_por_nivel[nivel].append(url)
        
        print("üìä DISTRIBUI√á√ÉO POR N√çVEIS:\n")
        for nivel in sorted(por_niveis.keys()):
            count = por_niveis[nivel]
            print(f"   N√≠vel {nivel}: {count:>6} URLs")
            print(f"   Exemplos:")
            for exemplo in exemplos_por_nivel[nivel][:3]:
                print(f"      ‚Ä¢ {exemplo}")
            print()
        
        # Procura padr√µes comuns de categoria
        print("\nüîç PROCURANDO CATEGORIAS REAIS:\n")
        
        categorias_candidatas = []
        
        for url in urls:
            path = urlparse(url).path
            segmentos = [s for s in path.split('/') if s]
            
            # Categorias geralmente:
            # - N√£o t√™m n√∫meros
            # - S√£o curtas
            # - N√£o t√™m h√≠fens com n√∫meros
            if len(segmentos) == 1:
                seg = segmentos[0]
                # N√£o tem n√∫meros no final
                if not re.search(r'-\d+$', seg):
                    # N√£o √© muito longa (categorias s√£o curtas)
                    if len(seg) < 30:
                        categorias_candidatas.append(url)
        
        print(f"‚úÖ Encontradas {len(categorias_candidatas)} categorias candidatas\n")
        print("üìÇ Primeiras 20 categorias:")
        for cat in categorias_candidatas[:20]:
            print(f"   ‚Ä¢ {cat}")
        
        # Testa se categorias t√™m produtos
        print(f"\n\nüß™ TESTANDO 3 CATEGORIAS:\n")
        
        from bs4 import BeautifulSoup
        from urllib.parse import urljoin
        
        for i, cat_url in enumerate(categorias_candidatas[:3], 1):
            print(f"[{i}] {cat_url}")
            
            try:
                r = await client.get(cat_url, follow_redirects=True)
                soup = BeautifulSoup(r.text, 'html.parser')
                
                # Conta produtos
                produtos = []
                for link in soup.find_all('a', href=True):
                    href = link.get('href')
                    if '/produto/' in href:
                        url = urljoin(cat_url, href)
                        produtos.append(url)
                
                produtos = list(set(produtos))
                print(f"   ‚úÖ {len(produtos)} produtos encontrados")
                
                if len(produtos) > 0:
                    print(f"   üì¶ Exemplos:")
                    for p in produtos[:3]:
                        print(f"      ‚Ä¢ {p.split('/')[-1][:60]}")
                
            except Exception as e:
                print(f"   ‚ùå Erro: {e}")
            
            print()

asyncio.run(investigar_sitemap())
