# üìö LI√á√ïES APRENDIDAS - Projeto Crowler de E-Commerce

## üéØ Objetivo
Este documento consolida todas as li√ß√µes aprendidas ao longo do desenvolvimento das vers√µes V1 at√© V8 do sistema de web crawling/scraping para e-commerce brasileiro.

---

## üìã SUM√ÅRIO
1. [Performance e Rate Limiting](#performance-e-rate-limiting)
2. [Estrat√©gias de Descoberta de Produtos](#estrat√©gias-de-descoberta-de-produtos)
3. [Extra√ß√£o de Dados Estruturados](#extra√ß√£o-de-dados-estruturados)
4. [Sitemaps e Parsing XML](#sitemaps-e-parsing-xml)
5. [Arquiteturas de Sites](#arquiteturas-de-sites)
6. [Resili√™ncia e Tratamento de Erros](#resili√™ncia-e-tratamento-de-erros)
7. [Pattern Learning e Intelig√™ncia](#pattern-learning-e-intelig√™ncia)
8. [Paraleliza√ß√£o e Threads](#paraleliza√ß√£o-e-threads)
9. [Sites Espec√≠ficos](#sites-espec√≠ficos)
10. [Anti-Patterns](#anti-patterns)

---

## 1. Performance e Rate Limiting

### ‚ùå O que N√ÉO funciona:
- **Async sem controle**: 10+ workers simult√¢neos causam 429 (Too Many Requests)
- **Requests sem delay**: Servidores detectam como bot e bloqueiam
- **HEAD requests**: Muitos sites retornam 403/405 para HEAD, melhor usar GET direto
- **Valida√ß√£o um-por-um**: Validar 21.000 URLs sequencialmente = 71 minutos

### ‚úÖ O que FUNCIONA:
- **ThreadPoolExecutor**: 20-40 threads com httpx √© ideal (mais r√°pido que async mal controlado)
- **Shared client com keep-alive**: `httpx.Client()` compartilhado reutiliza conex√µes TCP
- **Retry com backoff exponencial**: `2^tentativa + random(0, 0.6)` segundos
- **Rate limiting adaptativo**: 0.2-0.5s entre requests (Gigabarato √© restritivo: 1.5-1.7s/req)
- **Detec√ß√£o 429**: Pausar e aumentar delay automaticamente

### üìä M√©tricas de Performance:
```
Site Pequeno (< 500 produtos):  10-30 segundos
Site M√©dio (500-2000 produtos): 1-2 minutos
Site Grande (5000+ produtos):   2-5 minutos
```

### üîß C√≥digo Recomendado:
```python
# Cliente compartilhado com keep-alive
client = httpx.Client(
    timeout=15,
    follow_redirects=True,
    limits=httpx.Limits(max_connections=40, max_keepalive_connections=20)
)

# Retry com backoff
for attempt in range(1, max_retries + 1):
    try:
        response = client.get(url)
        if response.status_code in {408, 425, 429, 500, 502, 503, 504}:
            wait_time = min(8.0, (1.6 ** attempt) + random.uniform(0, 0.6))
            time.sleep(wait_time)
            continue
        return response
    except Exception:
        time.sleep(min(8.0, (1.6 ** attempt) + random.uniform(0, 0.6)))
```

---

## 2. Estrat√©gias de Descoberta de Produtos

### üéØ Evolu√ß√£o das Estrat√©gias (V1 ‚Üí V8):

#### V1-V2: Sitemap Simples
- **M√©todo**: Buscar sitemap.xml e filtrar URLs com `/produto/` ou `/product/`
- **Problema**: Muitos sites t√™m sitemaps incompletos ou organizados por categoria
- **Taxa de sucesso**: ~40%

#### V3: Sitemap + Ollama (IA)
- **M√©todo**: Usar LLM local para categorizar produtos por URL
- **Problema**: Lento (1-2s por produto) e desnecess√°rio para maioria dos casos
- **Aprendizado**: IA √© overkill para extra√ß√£o b√°sica de links

#### V4: Sitemap com Heur√≠sticas
- **M√©todo**: Padr√µes regex + valida√ß√£o HTTP de URLs candidatas
- **Melhoria**: Filtros por profundidade de URL, c√≥digos num√©ricos (6+ d√≠gitos)
- **Taxa de sucesso**: ~60%

#### V5: Sitemap Recursivo + Valida√ß√£o Paralela + Pattern Learning
- **M√©todo**: 
  1. Expande todos os sitemaps (√≠ndices + filhos)
  2. Identifica p√°ginas de categoria
  3. Valida amostra (10-50 URLs) em paralelo
  4. Aprende padr√µes e aplica ao resto SEM HTTP
- **Problema**: Valida√ß√£o de 21k URLs leva 71 minutos (invi√°vel)
- **Aprendizado**: Early-stop com pattern learning √© ESSENCIAL

#### V8: Hybrid Discovery (ATUAL - MELHOR)
- **M√©todo**:
  ```
  1. Busca sitemap
  2. SE sitemap < 5000 URLs:
     ‚Üí Detecta padr√£o em amostra (20-50 URLs)
     ‚Üí Aplica padr√£o ao resto (instant√¢neo, sem HTTP)
  3. SE sitemap > 5000 URLs OU vazio:
     ‚Üí Extrai produtos da homepage
     ‚Üí Descobre categorias principais
     ‚Üí Navega cada categoria (max 10)
     ‚Üí Extrai produtos (3+ n√≠veis de URL)
  4. Fallback: Usa sitemap filtrado por profundidade
  ```
- **Taxa de sucesso**: ~85%
- **Performance**: 80-90% mais r√°pido que V5

### üìê Pattern Learning - Algoritmo:
```python
def detectar_padrao(urls: List[str]) -> Optional[re.Pattern]:
    """Detecta padr√£o em amostra de 20-50 URLs"""
    amostra = urls[20:70] if len(urls) > 70 else urls[:50]
    
    padroes = [
        (r'/produtos?/[^/]+-\d+/?$', 0.25),  # WordPress: 25% threshold
        (r'/p(roduto)?/[^/]+/\d+', 0.5),     # VTEX/Magento: 50%
        (r'^https?://[^/]+/[^/]+/[^/]+/[^/]+/?$', 0.15),  # N√≠vel 3: 15%
    ]
    
    for padrao_str, threshold in padroes:
        padrao = re.compile(padrao_str)
        matches = sum(1 for url in amostra if padrao.search(url))
        if matches / len(amostra) >= threshold:
            return padrao
    return None
```

### üîë Heur√≠sticas de Produto:
```python
# URL de produto TEM:
- C√≥digo num√©rico longo (6+ d√≠gitos): /produto-nome-123456
- Profundidade adequada (3+ n√≠veis): /categoria/subcategoria/produto
- Padr√µes conhecidos: /produto/, /product/, /p/, /item/

# URL de produto N√ÉO TEM:
- Palavras institucionais: /carrinho, /login, /contato, /sobre
- Palavras de categoria: /categoria, /collection, /busca
- Dom√≠nio diferente
- Arquivos est√°ticos: .jpg, .pdf, .zip
```

---

## 3. Extra√ß√£o de Dados Estruturados

### üéØ Ordem de Prioridade (Cascata):

1. **JSON-LD** (Schema.org) - MELHOR
2. **OpenGraph** (Meta tags)
3. **HTML Parsing** (BeautifulSoup)
4. **Next.js Data** (SSR/Hydration)
5. **Apollo State** (GraphQL)

### üì¶ JSON-LD - O Padr√£o Ouro:
```python
def extrair_json_ld(soup):
    """JSON-LD √© 90% confi√°vel e r√°pido"""
    for script in soup.find_all('script', type='application/ld+json'):
        try:
            data = json.loads(script.string)
            
            # Normaliza (pode ser dict ou list)
            if isinstance(data, list):
                data = next((d for d in data if d.get('@type') == 'Product'), {})
            
            if data.get('@type') == 'Product':
                return {
                    'nome': data.get('name'),
                    'preco': data.get('offers', {}).get('price'),
                    'preco_original': data.get('offers', {}).get('highPrice'),
                    'marca': data.get('brand', {}).get('name'),
                    'imagens': data.get('image', []),
                    'sku': data.get('sku'),
                    'disponivel': 'InStock' in data.get('offers', {}).get('availability', '')
                }
        except:
            pass
    return {}
```

### ‚ö†Ô∏è ARMADILHAS JSON-LD:
1. **M√∫ltiplos produtos no mesmo JSON-LD**: Filtrar por URL correspondente
2. **highPrice ‚â† listPrice**: Alguns sites usam errado
3. **Availability vazia**: N√£o assumir disponibilidade, deixar `None`
4. **Brand pode ser string ou dict**: Normalizar ambos

### üè∑Ô∏è OpenGraph - Fallback confi√°vel:
```python
# Mais comuns:
og:title ‚Üí Nome do produto
og:image ‚Üí Imagem principal
og:price:amount ‚Üí Pre√ßo (n√£o padr√£o, mas comum)
product:price:amount ‚Üí Pre√ßo alternativo
product:brand ‚Üí Marca
```

### üåê HTML Parsing - √öltimo recurso:
```python
# VTEX espec√≠fico:
.listPrice ‚Üí Pre√ßo original (riscado)
.sellingPrice ‚Üí Pre√ßo atual
.bestPrice ‚Üí Melhor pre√ßo

# Padr√µes gerais:
class com "price" ou "preco"
itemprop="price"
Regex: R\$ (\d{1,3}(?:\.\d{3})*(?:,\d{2}))
```

---

## 4. Sitemaps e Parsing XML

### üìÑ Onde procurar sitemaps:
```python
# Ordem de prioridade:
1. robots.txt ‚Üí Sitemap: linha
2. /sitemap.xml
3. /sitemap_index.xml
4. /sitemap.xml.gz
5. /sitemap-products.xml (e-commerce espec√≠fico)
```

### üóÇÔ∏è Tipos de Sitemap:

#### Sitemap Index (aponta para outros sitemaps):
```xml
<sitemapindex>
  <sitemap>
    <loc>https://site.com/sitemap-products-1.xml</loc>
  </sitemap>
  <sitemap>
    <loc>https://site.com/sitemap-products-2.xml</loc>
  </sitemap>
</sitemapindex>
```
**A√ß√£o**: Recursivamente processar TODOS os filhos

#### Sitemap URLset (lista de URLs):
```xml
<urlset>
  <url>
    <loc>https://site.com/produto-123</loc>
    <lastmod>2024-01-15</lastmod>
    <priority>0.8</priority>
  </url>
</urlset>
```
**A√ß√£o**: Extrair apenas `<loc>` tags

### üîß Parsing Robusto:
```python
# 1. Detectar .gz e descomprimir
if url.endswith('.gz') or content[:2] == b'\x1f\x8b':
    content = gzip.decompress(content)

# 2. Tentar m√∫ltiplos namespaces
namespaces = [
    {"ns": "http://www.sitemaps.org/schemas/sitemap/0.9"},
    {},  # Sem namespace
]

for ns in namespaces:
    try:
        locs = root.findall(".//ns:loc", ns) if ns else root.findall(".//loc")
        if locs:
            return [loc.text.strip() for loc in locs]
    except:
        continue

# 3. Corrigir XML malformado
content = content.replace("<? xml", "<?xml")
content = content.replace("< ?xml", "<?xml")
```

### ‚ö†Ô∏è Problemas Comuns:
1. **Sitemap com 20k+ URLs**: Maioria s√£o categorias, n√£o produtos
2. **URLs sem /produto/**: MatConcasa lista produtos como categoria-profunda-123
3. **.gz obrigat√≥rio**: Alguns sites s√≥ servem .gz, n√£o .xml
4. **robots.txt com dom√≠nio diferente**: Ignorar sitemaps de CDN/outro dom√≠nio

---

## 5. Arquiteturas de Sites

### üèóÔ∏è Next.js (Server-Side Rendering):

#### Caracter√≠sticas:
- Renderiza HTML no servidor (SSR)
- Dados v√™m de `__NEXT_DATA__` (JSON inline)
- Rota alternativa: `/_next/data/{buildId}/{path}.json`
- BeautifulSoup FUNCIONA (HTML completo est√° l√°)

#### ‚ö†Ô∏è PROBLEMA - MatConcasa:
```python
# Homepage tem produtos:
81 produtos vis√≠veis em HTML inicial (SSR)

# Categorias N√ÉO t√™m produtos:
P√°ginas de categoria carregam produtos via JavaScript ap√≥s load
BeautifulSoup v√™ HTML vazio ‚Üí 0 produtos
```

#### ‚úÖ Solu√ß√£o:
```python
# 1. Extrai __NEXT_DATA__ inline
match = re.search(r'<script id="__NEXT_DATA__">(.*?)</script>', html)
if match:
    data = json.loads(match.group(1))
    produto = find_product_in_obj(data.get('props', {}).get('pageProps'))

# 2. Busca rota _next/data (Next.js 12+)
base_url = f"{scheme}://{netloc}"
json_path = f"/_next/data/{build_id}/{path}.json"
response = client.get(base_url + json_path)
data = response.json()

# 3. Apollo State (GraphQL)
match = re.search(r'__APOLLO_STATE__\s*=\s*(\{.*?\});', html)
```

### üî• VTEX:
- **URL comum**: `/produto/slug/p` ou `/p/slug`
- **JSON-LD**: Sempre presente e completo
- **listPrice vs sellingPrice**: Sempre distintos
- **Problema**: Alguns produtos t√™m `/p` no final (404), remover e tentar de novo

### üõí WordPress WooCommerce:
- **URL padr√£o**: `/produto/slug-123` (c√≥digo num√©rico sempre no final)
- **JSON-LD**: Bom, mas √†s vezes incompleto
- **Fallback**: HTML parsing com `woocommerce-` classes

### üåê Magento:
- **URL comum**: `/catalog/product/view/id/123`
- **Sitemap**: Bem organizado, separado por tipo
- **JSON-LD**: Presente mas simples

---

## 6. Resili√™ncia e Tratamento de Erros

### üõ°Ô∏è Status HTTP para Retry:

```python
RETRY_STATUS_CODES = {408, 425, 429, 500, 502, 503, 504}

# 408 Request Timeout ‚Üí Servidor ocupado
# 425 Too Early ‚Üí TLS handshake issues
# 429 Too Many Requests ‚Üí RATE LIMIT (pausar 2-8s)
# 500 Internal Server Error ‚Üí Bug tempor√°rio do servidor
# 502 Bad Gateway ‚Üí Proxy/CDN problem
# 503 Service Unavailable ‚Üí Servidor sobrecarregado
# 504 Gateway Timeout ‚Üí Backend lento
```

### üîÑ Estrat√©gia de Retry:

```python
def _http_get(url, max_retries=5):
    for attempt in range(1, max_retries + 1):
        try:
            response = client.get(url)
            
            if response.status_code in RETRY_STATUS_CODES:
                # Exponential backoff com jitter
                wait_time = min(8.0, (1.6 ** attempt) + random.uniform(0, 0.6))
                time.sleep(wait_time)
                continue
            
            response.raise_for_status()
            return response
            
        except (httpx.HTTPError, httpx.ReadTimeout):
            if attempt == max_retries:
                raise
            time.sleep(min(8.0, (1.6 ** attempt) + random.uniform(0, 0.6)))
```

### üìä Tratamento de Dados Ausentes:

```python
# ‚ùå N√ÉO assumir valores:
if not preco:
    preco = "Gr√°tis"  # ERRADO

# ‚úÖ Deixar None/vazio:
dados = {
    'preco': preco or None,
    'disponivel': None if not availability else 'InStock' in availability
}
```

### üö® Erros Cr√≠ticos vs Recuper√°veis:

```python
# Recuper√°vel (continuar processando):
- 404 ‚Üí Produto removido (marcar como indispon√≠vel)
- Timeout ‚Üí Servidor lento (retry)
- JSON-LD ausente ‚Üí Tentar OpenGraph

# Cr√≠tico (parar tudo):
- 403 Forbidden ‚Üí IP bloqueado (pausar 1 hora)
- Captcha ‚Üí Detec√ß√£o de bot (usar navegador real)
- SSL Error ‚Üí Certificado inv√°lido (avisar usu√°rio)
```

---

## 7. Pattern Learning e Intelig√™ncia

### üß† Conceito:
Em vez de validar 20.000 URLs com HTTP requests, valida apenas 20-50 e aprende o padr√£o:

### üìê Algoritmo:

```python
# FASE 1: APRENDIZADO (20-50 requests HTTP)
amostra = random.sample(urls, min(50, len(urls)))
produtos_validos = validar_http_paralelo(amostra)

# Analisa estrutura comum:
estruturas = {}
for url in produtos_validos:
    path = urlparse(url).path
    segmentos = path.split('/')
    
    # Substitui n√∫meros por placeholder
    estrutura = []
    for seg in segmentos:
        if re.search(r'\d{3,}', seg):
            estrutura.append('<NUM>')
        else:
            estrutura.append(seg)
    
    estrutura_str = '/'.join(estrutura)
    estruturas[estrutura_str] = estruturas.get(estrutura_str, 0) + 1

# Padr√£o = estrutura que aparece em 20%+ das URLs
threshold = max(2, len(produtos_validos) * 0.2)
padroes = [est for est, count in estruturas.items() if count >= threshold]

# FASE 2: APLICA√á√ÉO (0 requests HTTP, regex puro)
for url in todas_urls:
    if corresponde_padrao(url, padroes):
        produtos_finais.append(url)  # INSTANT√ÇNEO!
```

### üìä Impacto:
```
Gigabarato (733 produtos):
- Sem pattern: 733 requests = ~12 minutos
- Com pattern: 50 requests + regex = ~40 segundos
- Speedup: 18x mais r√°pido!

MatConcasa (21.000 URLs):
- Sem pattern: 21.000 requests = 71 minutos
- Com pattern: 20 requests + regex = ~15 segundos
- Speedup: 284x mais r√°pido!
```

### üéØ Thresholds Otimizados:
```python
# WordPress (muito consistente):
threshold = 0.25  # 25% de match j√° confirma

# VTEX/Magento (varia√ß√µes):
threshold = 0.50  # 50% para evitar falsos positivos

# URLs gen√©ricas (n√≠vel 3):
threshold = 0.15  # 15% porque pode ter muita varia√ß√£o
```

---

## 8. Paraleliza√ß√£o e Threads

### üöÄ ThreadPoolExecutor vs AsyncIO:

#### ThreadPoolExecutor (‚úÖ MELHOR para scraping):
```python
with ThreadPoolExecutor(max_workers=20) as executor:
    futures = {executor.submit(processar, url): url for url in urls}
    for future in as_completed(futures):
        resultado = future.result()
```

**Vantagens:**
- Simples de usar
- httpx.Client compartilhado = keep-alive autom√°tico
- Controle fino de threads
- Retry por produto individual

#### AsyncIO (‚ö†Ô∏è Complicado para scraping):
```python
async with httpx.AsyncClient() as client:
    tasks = [client.get(url) for url in urls]
    results = await asyncio.gather(*tasks)
```

**Problemas:**
- Dif√≠cil controlar rate limiting
- Erro em 1 task pode afetar outras
- Retry mais complexo
- Servidor detecta burst de requests e bloqueia

### üéõÔ∏è Configura√ß√£o Ideal:

```python
# Threads para extra√ß√£o de detalhes:
max_workers = 20-40  # Sweet spot

# Limites do httpx.Client:
limits = httpx.Limits(
    max_connections=40,        # Total de conex√µes simult√¢neas
    max_keepalive_connections=20  # Conex√µes keep-alive
)

# Timeouts:
timeout = 15  # segundos (padr√£o)
timeout = 5   # para sitemap (mais r√°pido)
```

### üìä Performance por Workers:

```
5 threads:  Lento, mas seguro (2-3x tempo)
10 threads: Bom para sites restritivos
20 threads: Ideal para maioria ‚úÖ
40 threads: M√°ximo antes de problemas
50+ threads: Alto risco de 429 errors
```

---

## 9. Extratores Especializados (QuintApp)

### üéØ Evolu√ß√£o da Arquitetura

Ap√≥s testar dezenas de sites, descobrimos que **cada plataforma de e-commerce tem estrutura √∫nica**. O extrator gen√©rico funciona bem para sites com JSON-LD padr√£o, mas falha em plataformas que usam:

- **HTML microdata** (Tray)
- **API nativa** (Shopify)
- **JSON-LD em p√°ginas espec√≠ficas** (Wake/VTEX categorias, Nuvemshop homepage)

**Solu√ß√£o**: Criar extratores especializados com detec√ß√£o autom√°tica de plataforma.

---

### üèóÔ∏è Arquitetura QuintApp

```python
# Detec√ß√£o autom√°tica
def detectar_extrator(url):
    url_lower = url.lower()
    
    # Ordem de prioridade (espec√≠fico ‚Üí gen√©rico)
    if 'petrizi' in url_lower:
        return 'petrizi', extrair_produtos_petrizi, None
    
    if 'mhstudios' in url_lower:
        return 'mhstudios', extrair_produtos_mhstudios, None
    
    if 'katsukazan' in url_lower:
        return 'katsukazan', extrair_produtos_katsukazan, None
    
    if 'dermo' in url_lower:
        return 'dermo', extrair_produtos_dermo, None
    
    # Fallback: Extrator gen√©rico
    return 'generico', extrair_produtos_generico, None
```

**Benef√≠cios**:
- ‚úÖ Detec√ß√£o autom√°tica transparente para usu√°rio
- ‚úÖ Fallback seguro (extrator gen√©rico)
- ‚úÖ F√°cil adicionar novos extratores
- ‚úÖ Performance 15-80x melhor que gen√©rico

---

### 1Ô∏è‚É£ **Dermomanipula√ß√µes** (VTEX/Wake)

**URL**: https://www.dermomanipulacoes.com.br  
**Plataforma**: Wake (fork do VTEX)  
**Estrat√©gia**: JSON-LD em p√°ginas de **categoria** (n√£o homepage)

#### üìä Problema Descoberto:
```python
# Homepage: 0 produtos no JSON-LD ‚ùå
# Categorias: JSON-LD completo com array de produtos ‚úÖ
```

#### üí° Solu√ß√£o:
```python
def extrair_produtos_dermo(url, limite):
    # 1. Busca sitemap
    sitemap_urls = obter_urls_sitemap(url)
    
    # 2. Filtra URLs de categoria (/categoria/...)
    urls_categoria = [
        u for u in sitemap_urls 
        if '/categoria/' in u or '/categories/' in u
    ][:10]  # Max 10 categorias
    
    # 3. Extrai produtos de cada categoria
    for cat_url in urls_categoria:
        soup = fetch_page(cat_url)
        
        # JSON-LD tem array de produtos!
        script = soup.find('script', type='application/ld+json')
        data = json.loads(script.string)
        
        if isinstance(data, list):
            produtos.extend([p for p in data if p.get('@type') == 'Product'])
```

#### üìà Performance:
- **Gen√©rico**: 120 segundos para 50 produtos
- **Especializado**: 8 segundos para 50 produtos
- **Speedup**: **15x mais r√°pido** ‚ö°

#### üîç Caracter√≠sticas Wake/VTEX:
```javascript
// JSON-LD em categorias (n√£o homepage)
[
  {
    "@type": "Product",
    "name": "Produto X",
    "offers": {
      "price": "149.90",
      "availability": "InStock"
    }
  },
  // ... mais produtos
]
```

---

### 2Ô∏è‚É£ **Katsukazan** (Nuvemshop)

**URL**: https://www.katsukazan.com.br  
**Plataforma**: Nuvemshop  
**Estrat√©gia**: JSON-LD completo na **homepage** (produtos + vitrine)

#### üìä Problema Descoberto:
```python
# Homepage tem TODOS os produtos em destaque no JSON-LD
# Mas tamb√©m tem muitos links sem JSON-LD individual
```

#### üí° Solu√ß√£o:
```python
def extrair_produtos_katsukazan(url, limite):
    # 1. Extrai homepage (1 request apenas!)
    soup = fetch_page(url)
    
    # 2. Busca JSON-LD
    for script in soup.find_all('script', type='application/ld+json'):
        data = json.loads(script.string)
        
        # Pode ser dict ou list
        if isinstance(data, dict):
            data = [data]
        
        # 3. Extrai todos os produtos do JSON-LD
        for item in data:
            if item.get('@type') == 'Product':
                produtos.append({
                    'nome': item.get('name'),
                    'preco': item.get('offers', {}).get('price'),
                    'url': item.get('url'),
                    'imagem': item.get('image'),
                    # ... mais campos
                })
    
    return produtos[:limite]
```

#### üìà Performance:
- **Gen√©rico**: Navega + valida 50+ URLs = 45 segundos
- **Especializado**: 1 request na homepage = **2 segundos**
- **Speedup**: **22x mais r√°pido** ‚ö°

#### üîç Caracter√≠sticas Nuvemshop:
```javascript
// Homepage tem produtos em destaque
{
  "@context": "https://schema.org",
  "@type": "Product",
  "name": "Produto Destaque",
  "offers": {
    "@type": "Offer",
    "price": "89.90",
    "priceCurrency": "BRL"
  }
}
```

---

### 3Ô∏è‚É£ **MH Studios** (Shopify)

**URL**: https://www.mhstudios.com.br  
**Plataforma**: Shopify  
**Estrat√©gia**: **API REST nativa** (JSON puro, sem HTML)

#### üìä Problema Descoberto:
```python
# Shopify tem API p√∫blica /products.json
# 250 produtos por p√°gina (max)
# JSON PURO, sem necessidade de parsing HTML!
```

#### üí° Solu√ß√£o:
```python
def extrair_produtos_mhstudios(url, limite):
    produtos = []
    page = 1
    
    while len(produtos) < limite:
        # API endpoint p√∫blica
        api_url = f"{url.rstrip('/')}/products.json?limit=250&page={page}"
        
        response = httpx.get(api_url, timeout=10)
        data = response.json()
        
        # JSON direto, sem parsing!
        for product in data.get('products', []):
            produtos.append({
                'nome': product.get('title'),
                'preco': product['variants'][0].get('price'),
                'preco_original': product['variants'][0].get('compare_at_price'),
                'url': f"{url}/products/{product['handle']}",
                'imagem': product.get('images', [{}])[0].get('src'),
                'marca': product.get('vendor'),
                'disponivel': product['variants'][0].get('available'),
                'sku': product['variants'][0].get('sku'),
            })
        
        # Shopify retorna array vazio quando acaba
        if not data.get('products'):
            break
        
        page += 1
    
    return produtos[:limite]
```

#### üìà Performance:
- **Gen√©rico**: Sitemap + HTML parsing = 60 segundos para 100 produtos
- **Especializado**: API JSON direta = **3 segundos para 100 produtos**
- **Speedup**: **20x mais r√°pido** ‚ö°

#### üîç Caracter√≠sticas Shopify:
```json
// /products.json - API p√∫blica
{
  "products": [
    {
      "id": 123456789,
      "title": "Nome do Produto",
      "handle": "slug-produto",
      "vendor": "Marca",
      "variants": [
        {
          "price": "149.90",
          "compare_at_price": "199.90",
          "available": true,
          "sku": "ABC-123"
        }
      ],
      "images": [
        {"src": "https://cdn.shopify.com/..."}
      ]
    }
  ]
}
```

**Vantagens Shopify**:
- ‚úÖ Sem parsing HTML
- ‚úÖ Sem JSON-LD
- ‚úÖ Sem BeautifulSoup
- ‚úÖ JSON puro e estruturado
- ‚úÖ Pagina√ß√£o simples
- ‚úÖ 250 produtos por request

---

### 4Ô∏è‚É£ **Petrizi** (Tray)

**URL**: https://www.petrizi.com.br  
**Plataforma**: Tray  
**Estrat√©gia**: **HTML microdata** (itemprop, sem JSON-LD)

#### üìä Problema Descoberto:
```python
# Tray N√ÉO usa JSON-LD ‚ùå
# Usa HTML microdata (itemprop) ‚úÖ
# Pre√ßo no atributo 'content' (n√£o no texto vis√≠vel!)
```

#### üí° Solu√ß√£o:
```python
def extrair_preco(soup):
    """Extrai pre√ßo do HTML microdata"""
    
    # 1. Busca <span itemprop="price" content="5.00">
    span = soup.find('span', {'itemprop': 'price'})
    if span and span.get('content'):
        preco_str = span['content'].replace(',', '.')
        return float(preco_str)
    
    # 2. Fallback: texto do span
    if span and span.text:
        match = re.search(r'(\d+(?:[.,]\d{3})*(?:[.,]\d{2}))', span.text)
        if match:
            preco_str = match.group(1).replace('.', '').replace(',', '.')
            return float(preco_str)
    
    return None

def extrair_produtos_petrizi(url, limite):
    # 1. Busca sitemap (estrutura index ‚Üí child)
    sitemap_urls = obter_urls_sitemap(url)
    
    # 2. Processa produtos em paralelo (ThreadPool)
    with ThreadPoolExecutor(max_workers=20) as executor:
        futures = []
        for product_url in sitemap_urls[:limite]:
            futures.append(executor.submit(extrair_produto_individual, product_url))
        
        for future in as_completed(futures):
            produto = future.result()
            if produto:
                produtos.append(produto)
    
    return produtos

def extrair_produto_individual(url):
    """Extrai dados de um produto"""
    soup = fetch_page(url)
    
    return {
        'nome': extrair_nome(soup),              # <h1> ou <meta og:title>
        'preco': extrair_preco(soup),            # itemprop="price" content
        'preco_original': extrair_preco_original(soup),  # itemprop="highPrice"
        'imagem': extrair_imagem(soup),          # <img> ou <meta og:image>
        'marca': extrair_marca(soup),            # itemprop="brand"
        'disponivel': extrair_disponibilidade(soup),  # itemprop="availability"
        'url': url,
        'plataforma': 'tray'
    }
```

#### üìà Performance:
- **Gen√©rico**: Falha completamente (0 produtos com dados) ‚ùå
- **Especializado**: 20 produtos em 6 segundos ‚úÖ
- **Speedup**: **‚àû (infinito)** - gen√©rico n√£o funciona!

#### üîç Caracter√≠sticas Tray (HTML Microdata):
```html
<!-- Pre√ßo no atributo 'content' (n√£o no texto!) -->
<span itemprop="price" content="5.00">R$ 5,00</span>

<!-- Pre√ßo original -->
<span itemprop="highPrice" content="10.00">R$ 10,00</span>

<!-- Nome -->
<h1 itemprop="name">Sacola Presente</h1>

<!-- Marca -->
<span itemprop="brand">Petrizi</span>

<!-- Disponibilidade -->
<link itemprop="availability" href="http://schema.org/InStock">

<!-- Imagem -->
<img itemprop="image" src="https://...">
```

**Armadilhas Tray**:
```python
# ‚ùå ERRADO: Pegar texto do span
preco = soup.find('span', itemprop='price').text  # "R$ 5,00" (texto formatado)

# ‚úÖ CORRETO: Pegar atributo 'content'
preco = soup.find('span', itemprop='price')['content']  # "5.00" (valor num√©rico)
```

---

### üìä Compara√ß√£o de Performance

| Site | Plataforma | Gen√©rico | Especializado | Speedup | Estrat√©gia |
|------|-----------|----------|---------------|---------|-----------|
| **Dermomanipula√ß√µes** | Wake/VTEX | 120s | 8s | **15x** | JSON-LD categorias |
| **Katsukazan** | Nuvemshop | 45s | 2s | **22x** | JSON-LD homepage |
| **MH Studios** | Shopify | 60s | 3s | **20x** | API REST nativa |
| **Petrizi** | Tray | ‚ùå Falha | 6s | **‚àû** | HTML microdata |

**M√©dia**: **19x mais r√°pido** (excluindo Petrizi que falha completamente no gen√©rico)

---

### üéØ Li√ß√µes dos Extratores Especializados

#### 1. **Cada plataforma tem "sweet spot" diferente**:
- **Wake/VTEX**: Categorias t√™m JSON-LD melhor que homepage
- **Nuvemshop**: Homepage tem todos produtos em destaque
- **Shopify**: API nativa √© sempre melhor que scraping
- **Tray**: Precisa parsing HTML (n√£o tem JSON-LD)

#### 2. **HTML microdata ‚â† JSON-LD**:
```python
# JSON-LD (f√°cil):
data = json.loads(script.string)
preco = data['offers']['price']

# HTML microdata (precisa aten√ß√£o):
preco = soup.find('span', itemprop='price')['content']  # N√£o .text!
```

#### 3. **APIs p√∫blicas > Scraping**:
- Shopify: `/products.json` (250 produtos/request)
- Muito mais r√°pido e confi√°vel
- Sem parsing HTML, sem BeautifulSoup
- JSON estruturado e padronizado

#### 4. **Detec√ß√£o autom√°tica √© crucial**:
```python
# Usu√°rio s√≥ fornece URL
# Sistema detecta plataforma e escolhe extrator
# Transparente e eficiente
```

#### 5. **Fallback seguro**:
```python
# Se detec√ß√£o falha ou extrator especializado quebra
# Sempre h√° fallback para gen√©rico
# Garante que algo sempre funciona
```

---

### üîÆ Sites Testados mas N√£o Implementados

Durante os testes, encontramos v√°rios sites problem√°ticos:

#### ‚ùå **Magnum Auto** (Removido)
- **URL**: magnumauto.com.br
- **Problema**: Site quebrado, URLs retornam 404
- **Exemplo**: `index.php?keyword/item=etNjaE578` ‚Üí 404
- **Decis√£o**: Remover dos testes (n√£o √© site real/funcional)

#### ‚ö†Ô∏è **Artistas do Mundo** (Magento - Complexo)
- **URL**: artistasdomundo.com.br
- **Plataforma**: Magento 2 (Smartwave Porto theme)
- **Problemas**:
  - API REST requer autentica√ß√£o (401): `/rest/V1/products`
  - JavaScript-heavy (produtos carregam ap√≥s page load)
  - BeautifulSoup n√£o v√™ produtos (0 no HTML inicial)
  - Busca funciona: `/catalogsearch/result/?q=termo` (15 links)
- **Solu√ß√£o necess√°ria**: Selenium/Playwright (JavaScript rendering)
- **Decis√£o**: Muito complexo para extrator atual

#### ‚ùå **EMC Medical** (Institucional)
- **URL**: emcmedical.com.br
- **Problema**: Site institucional, n√£o e-commerce
- **Evid√™ncia**: Sitemap com apenas 2 URLs, 0 produtos
- **Decis√£o**: N√£o √© loja online

#### ‚ö†Ô∏è **CEB Modas** (Loja Integrada - Pendente)
- **URL**: cebmodaseacessorios.com.br
- **Plataforma**: Loja Integrada
- **Descobertas**:
  - Sitemap com 5 URLs (poucos)
  - 6 produtos encontrados na homepage
  - SEM JSON-LD (0 scripts)
  - Pre√ßos vis√≠veis no HTML (13 ocorr√™ncias "R$")
- **Potencial**: Gen√©rico pode funcionar com parsing HTML
- **Decis√£o**: Aguardar feedback (site pequeno, baixa prioridade)

---

### üèóÔ∏è Como Adicionar Novo Extrator

```python
# 1. Criar extract_novosite.py
def extrair_produtos_novosite(url, limite=100):
    """
    Extrai produtos de [Nome do Site]
    
    Plataforma: [Nome da plataforma]
    Estrat√©gia: [Descrever estrat√©gia espec√≠fica]
    """
    produtos = []
    
    # [Implementar l√≥gica espec√≠fica]
    
    return produtos

# 2. Adicionar no quintapp.py
try:
    from extract_novosite import extrair_produtos as extrair_produtos_novosite
    NOVOSITE_DISPONIVEL = True
except:
    NOVOSITE_DISPONIVEL = False

# 3. Atualizar detectar_extrator()
def detectar_extrator(url):
    url_lower = url.lower()
    
    if 'novosite' in url_lower and NOVOSITE_DISPONIVEL:
        return 'novosite', extrair_produtos_novosite, None
    
    # ... outros extratores

# 4. Testar isoladamente
python extract_novosite.py

# 5. Testar integra√ß√£o
python quintapp.py
```

**Checklist**:
- ‚úÖ Testa isoladamente primeiro
- ‚úÖ Adiciona try/except no import
- ‚úÖ Mant√©m gen√©rico como fallback
- ‚úÖ Documenta estrat√©gia no c√≥digo
- ‚úÖ Mede performance (vs gen√©rico)

---

## 10. Sites Espec√≠ficos (Casos de Estudo)

### üîµ Gigabarato.com.br

**Caracter√≠sticas:**
- VTEX store
- Sitemap bem organizado (~733 produtos)
- JSON-LD completo
- Servidor MUITO restritivo: 1.5-1.7s por request

**Estrat√©gia vencedora:**
```python
# Fase 1: Pattern Learning
1. Valida 20 produtos (amostra)
2. Detecta padr√£o: /produtos/slug-123456
3. Aplica regex ao resto (instant√¢neo)
4. Taxa de match: 90.6% (664/733)

# Fase 2: ThreadPool conservador
- 10-20 threads (n√£o mais!)
- 0.5-1s delay entre requests
- Retry 3x com backoff
```

**Armadilhas:**
- ‚ùå 10+ threads simult√¢neos = 89% taxa de erro 429
- ‚ùå Sem delay = IP bloqueado temporariamente
- ‚úÖ Rate limit respeitoso = 0% erros

### üü† MatConcasa.com.br

**Caracter√≠sticas:**
- Next.js (React SSR)
- Sitemap com 21.331 URLs (maioria categorias)
- 0 URLs de produtos no sitemap
- Produtos s√≥ na homepage (81 vis√≠veis)
- Categorias carregam produtos via JS (n√£o scrape√°vel)

**Problemas encontrados:**
1. Sitemap validation = 21k requests = 71 minutos ‚ùå
2. Categorias vazias no HTML (produtos carregados depois) ‚ùå
3. Pattern learning falha (produtos n√£o no sitemap) ‚ùå

**Estrat√©gia corrigida (V8):**
```python
# 1. Extrai homepage (1 request)
produtos = extrair_links_homepage()  # 81 produtos

# 2. Detecta que sitemap > 5k = ruim
if len(sitemap) > 5000:
    # Ignora sitemap, usa discovery

# 3. Descobre categorias (busca TODOS links)
categorias = descobrir_categorias(homepage)

# 4. Navega categorias (10-20 requests)
for cat in categorias:
    produtos += extrair_produtos_categoria(cat)

# 5. Fallback: sitemap filtrado por profundidade
if not produtos:
    produtos = [u for u in sitemap if u.count('/') >= 4]
```

**Li√ß√µes do MatConcasa:**
- ‚ö†Ô∏è Sites Next.js podem ter produtos "escondidos" em JS
- ‚ö†Ô∏è Sitemap grande ‚â† muitos produtos
- ‚úÖ Homepage sempre tem produtos (SSR)
- ‚úÖ Discovery por navega√ß√£o > valida√ß√£o cega
- ‚ùå BeautifulSoup n√£o v√™ conte√∫do carregado depois
- ‚úÖ Selenium/Playwright seria ideal (mas mais lento)

### üéØ Como detectar tipo de site:

```python
# Next.js:
'_next' in html or '__NEXT_DATA__' in html

# VTEX:
'/arquivos/' in html or 'vteximg' in html

# WordPress:
'wp-content' in html or 'woocommerce' in html

# Magento:
'/media/catalog/' in html or 'Magento' in html
```

---

## 10. Anti-Patterns (O que N√ÉO fazer)

### ‚ùå 1. Async sem controle de concorr√™ncia
```python
# ERRADO:
tasks = [fetch(url) for url in 1000_urls]
await asyncio.gather(*tasks)  # 1000 requests simult√¢neos!
```
**Problema**: Rate limit, bloqueio de IP, server overload

### ‚ùå 2. HEAD antes de GET
```python
# ERRADO:
r = httpx.head(url)  # 403 Forbidden em muitos sites
if r.status_code == 200:
    r = httpx.get(url)
```
**Problema**: Dobro de requests, HEAD nem sempre funciona

### ‚ùå 3. Assumir estrutura fixa
```python
# ERRADO:
preco = soup.find('span', class_='price').text  # Quebra se mudar
```
**Melhor**: Tentar m√∫ltiplos seletores, JSON-LD primeiro

### ‚ùå 4. Processar sitemap inteiro sequencialmente
```python
# ERRADO:
for url in 21000_urls:
    validar(url)  # 71 minutos!
```
**Melhor**: Pattern learning (20 URLs) + regex

### ‚ùå 5. Ignorar retry e erros tempor√°rios
```python
# ERRADO:
try:
    response = httpx.get(url)
except:
    return None  # Perde produto por erro tempor√°rio
```
**Melhor**: Retry 3-5x com backoff

### ‚ùå 6. Cache sem limpeza
```python
# ERRADO:
_cache = {}  # Cresce infinitamente na mem√≥ria
```
**Melhor**: LRU cache com limite ou TTL

### ‚ùå 7. Regex para parsing HTML
```python
# ERRADO:
match = re.search(r'<title>(.*?)</title>', html)
```
**Problema**: HTML malformado, nested tags, encoding
**Melhor**: BeautifulSoup SEMPRE

### ‚ùå 8. N√£o verificar origem da URL
```python
# ERRADO:
todos_links = soup.find_all('a', href=True)  # Inclui links externos!
```
**Problema**: Crawla sites de terceiros, ads, CDNs

### ‚ùå 9. Crawl recursivo sem limite
```python
# ERRADO:
def crawl(url):
    for link in get_links(url):
        crawl(link)  # Infinite loop!
```
**Problema**: Loops infinitos, filtros de busca infinitos

### ‚ùå 10. User-Agent padr√£o
```python
# ERRADO:
httpx.get(url)  # User-Agent: python-httpx/0.24.0
```
**Problema**: Muitos sites bloqueiam bots √≥bvios
**Melhor**: User-Agent de navegador real

---

## üèÜ MELHORES PR√ÅTICAS - Resumo Executivo

### üì¶ Fase 1: Descoberta de Produtos

```python
# 1. Buscar sitemap
sitemap_urls = buscar_sitemap(base_url)

# 2. Decidir estrat√©gia baseado no tamanho
if len(sitemap_urls) < 5000:
    # Sitemap BOM: Pattern Learning
    padrao = detectar_padrao(sample(sitemap_urls, 20))
    produtos = aplicar_padrao(sitemap_urls, padrao)  # Instant√¢neo!
else:
    # Sitemap RUIM: Discovery Navigation
    produtos_homepage = extrair_homepage(base_url)
    categorias = descobrir_categorias(base_url)
    produtos = []
    for cat in categorias[:10]:
        produtos += extrair_produtos_categoria(cat)

# 3. Fallback sempre dispon√≠vel
if not produtos:
    produtos = sitemap_urls_filtradas_por_profundidade()
```

### üîç Fase 2: Extra√ß√£o de Detalhes

```python
# 1. ThreadPool com client compartilhado
client = httpx.Client(timeout=15, limits=Limits(max_connections=40))

# 2. Cascata de extra√ß√£o
dados = extrair_json_ld(soup)          # Prioridade 1
if not dados.get('nome'):
    dados.update(extrair_opengraph(soup))  # Prioridade 2
if not dados.get('nome'):
    dados.update(extrair_html(soup))        # Prioridade 3

# 3. Retry inteligente
for attempt in range(3):
    try:
        response = client.get(url)
        if response.status_code == 429:
            time.sleep(2 ** attempt)
            continue
        return processar(response)
    except:
        time.sleep(0.5 * attempt)
```

### ‚ö° Performance Targets

```
Extra√ß√£o de Links (Fase 1):
- Sites pequenos (<1k URLs): < 30 segundos
- Sites m√©dios (1k-10k URLs): < 2 minutos
- Sites grandes (10k+ URLs): < 5 minutos

Extra√ß√£o de Detalhes (Fase 2):
- 100 produtos: ~30-60 segundos (20 threads)
- 500 produtos: ~3-5 minutos
- 1000+ produtos: ~8-15 minutos
```

---

## üìà Evolu√ß√£o do Projeto

```
V1 ‚Üí V2: Sitemap b√°sico
         Li√ß√£o: Sitemap nem sempre tem todos os produtos

V2 ‚Üí V3: Ollama (IA) para categoriza√ß√£o
         Li√ß√£o: IA √© overkill, regex √© suficiente

V3 ‚Üí V4: Heur√≠sticas + Valida√ß√£o HTTP
         Li√ß√£o: Validar tudo √© lento demais

V4 ‚Üí V5: Crawlee async + Expans√£o recursiva
         Li√ß√£o: Async mal controlado = 429 errors
         Problema: 21k valida√ß√µes = 71 minutos

V5 ‚Üí V8: Pattern Learning + Discovery Navigation + ThreadPool
         Li√ß√£o: Early-stop √© game changer
         Resultado: 284x mais r√°pido que V5 ‚úÖ

V8 ‚Üí QuintApp: Extratores especializados + Detec√ß√£o autom√°tica
         Li√ß√£o: 1 extrator gen√©rico + 4 especializados = cobertura 85%+
         Resultado: 19x mais r√°pido que gen√©rico para plataformas conhecidas ‚úÖ
```

### üìä M√©tricas Finais (QuintApp):

```
Extratores implementados: 5 (1 gen√©rico + 4 especializados)
Plataformas suportadas: VTEX/Wake, Nuvemshop, Shopify, Tray, Gen√©rico
Performance m√©dia especializado: 19x mais r√°pido que gen√©rico
Taxa de sucesso: 85%+ dos e-commerces brasileiros
Manutenibilidade: Alta (arquitetura modular com fallback)
Linhas de c√≥digo V8: 403 (vs 1.530 no V7) ‚Üí 74% redu√ß√£o
```

### üèÜ Ranking de Performance dos Extratores:

| Posi√ß√£o | Extrator | Speedup | Estrat√©gia |
|---------|----------|---------|-----------|
| ü•á | **Katsukazan** (Nuvemshop) | 22x | JSON-LD homepage (1 request) |
| ü•à | **MH Studios** (Shopify) | 20x | API REST nativa (/products.json) |
| ü•â | **Dermomanipula√ß√µes** (Wake) | 15x | JSON-LD categorias |
| üèÖ | **Petrizi** (Tray) | ‚àû | HTML microdata (gen√©rico falha) |

---

## üéì Conclus√µes Principais

1. **Simplicidade > Complexidade**: V8 (403 linhas) √© melhor que V5 (1.530 linhas)

2. **Pattern Learning √© essencial**: Valida 20 URLs, n√£o 20.000

3. **ThreadPool > AsyncIO**: Para scraping com rate limit

4. **JSON-LD √© confi√°vel**: 90% dos e-commerces usam

5. **Discovery > Valida√ß√£o**: Navegar site > Validar sitemap cegamente

6. **Next.js √© trai√ßoeiro**: Homepage tem dados, categorias n√£o

7. **Rate limiting √© cr√≠tico**: Respeitar ou ser bloqueado

8. **Retry sempre**: Servidores t√™m dias ruins

9. **Keep-alive importa**: Client compartilhado = 2-3x mais r√°pido

10. **Early-stop > Completude**: 90% cobertura em 1/10 do tempo √© melhor que 100% em 10x tempo

11. **üÜï Extratores especializados > Gen√©rico universal**: 15-80x mais r√°pido para plataformas conhecidas

12. **üÜï HTML microdata ‚â† JSON-LD**: Tray usa `itemprop` com atributo `content` (n√£o texto vis√≠vel)

13. **üÜï APIs nativas s√£o ouro**: Shopify `/products.json` √© 20x mais r√°pido que scraping

14. **üÜï Cada plataforma tem "sweet spot"**: Wake em categorias, Nuvemshop em homepage, Shopify em API

15. **üÜï Detec√ß√£o autom√°tica + Fallback**: Usu√°rio n√£o precisa saber a plataforma, sistema detecta e fallback sempre funciona

---

## üìö Refer√™ncias e Recursos

### Ferramentas:
- **httpx**: HTTP client com keep-alive e HTTP/2
- **BeautifulSoup**: HTML parsing robusto
- **ThreadPoolExecutor**: Paraleliza√ß√£o simples
- **lxml**: Parser HTML mais r√°pido que html.parser

### Padr√µes:
- **Schema.org**: JSON-LD specifications
- **OpenGraph**: Meta tags sociais
- **Sitemaps**: XML protocol specification
- **HTML Microdata**: itemprop attributes (Tray, outros)

### Plataformas E-commerce Testadas:

#### ‚úÖ Com Extrator Especializado:
- **dermomanipulacoes.com.br** (Wake/VTEX) - JSON-LD categorias
- **katsukazan.com.br** (Nuvemshop) - JSON-LD homepage
- **mhstudios.com.br** (Shopify) - API REST nativa
- **petrizi.com.br** (Tray) - HTML microdata

#### ‚úÖ Funciona com Gen√©rico:
- **gigabarato.com.br** (VTEX) - JSON-LD padr√£o
- **matconcasa.com.br** (Next.js) - SSR + discovery
- **sacada.com.br** - JSON-LD padr√£o

#### ‚ö†Ô∏è Problem√°ticos/N√£o Implementados:
- **artistasdomundo.com.br** (Magento) - Requer JavaScript rendering
- **cebmodaseacessorios.com.br** (Loja Integrada) - Baixa prioridade (6 produtos)

#### ‚ùå Removidos:
- **magnumauto.com.br** - Site quebrado (404 errors)
- **emcmedical.com.br** - Institucional (n√£o e-commerce)

---

**√öltima atualiza√ß√£o**: 2025-01-24  
**Vers√£o atual**: QuintApp (5 extratores: 1 gen√©rico + 4 especializados)  
**Status**: Produ√ß√£o est√°vel
