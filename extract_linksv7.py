#!/usr/bin/env python3
"""
EXTRACT LINKS V7 - Crawlee-Style Architecture
==============================================

Implementa extra√ß√£o de links de produtos usando padr√µes do Crawlee:
- RequestQueue com prioridade
- AdaptiveRateLimiter (AutoscaledPool)
- SessionPool para gerenciar cookies
- Extra√ß√£o inteligente de sitemaps e listagens
- Valida√ß√£o paralela com rate limiting

Fluxo:
  1. Busca sitemaps (XML, robots.txt)
  2. Aprende padr√µes de URLs de produto
  3. Filtra e valida produtos
  4. Retorna lista estruturada
"""

import asyncio
import httpx
import re
import time
from typing import List, Dict, Callable, Optional
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET


# ================================================================================================
# ADAPTIVE RATE LIMITER (AutoscaledPool)
# ================================================================================================
class AdaptiveRateLimiter:
    """Rate limiter adaptativo similar ao AutoscaledPool do Crawlee."""
    
    def __init__(self, requests_per_minute: int = 60, autoscale: bool = True):
        self.max_rpm = requests_per_minute
        self.current_rpm = requests_per_minute
        self.autoscale = autoscale
        
        # Controle de taxa
        self.tokens = []
        self.lock = asyncio.Lock()
        
        # M√©tricas
        self.recent_requests = 0
        self.recent_errors = 0
        self.recent_429s = 0
    
    async def acquire(self):
        """Aguarda at√© poder fazer pr√≥xima requisi√ß√£o."""
        async with self.lock:
            now = time.time()
            
            # Remove tokens antigos (>60s)
            self.tokens = [t for t in self.tokens if now - t < 60]
            
            # Se atingiu limite, aguarda
            while len(self.tokens) >= self.current_rpm:
                await asyncio.sleep(0.1)
                now = time.time()
                self.tokens = [t for t in self.tokens if now - t < 60]
            
            # Adiciona novo token
            self.tokens.append(now)
            
            # Jitter leve
            import random
            await asyncio.sleep(random.uniform(0, 0.05))
    
    def report_success(self):
        self.recent_requests += 1
        self._maybe_adjust()
    
    def report_error(self):
        self.recent_requests += 1
        self.recent_errors += 1
        self._maybe_adjust()
    
    def report_429(self):
        self.recent_requests += 1
        self.recent_429s += 1
        self._maybe_adjust()
    
    def _maybe_adjust(self):
        """Ajusta RPM automaticamente."""
        if not self.autoscale or self.recent_requests < 10:
            return
        
        error_rate = (self.recent_errors + self.recent_429s) / self.recent_requests
        
        if self.recent_429s > 2:
            self.current_rpm = max(20, int(self.current_rpm * 0.5))
        elif error_rate > 0.2:
            self.current_rpm = max(30, int(self.current_rpm * 0.7))
        elif error_rate < 0.05 and self.recent_429s == 0:
            self.current_rpm = min(self.max_rpm, int(self.current_rpm * 1.1))
        
        # Reset
        self.recent_requests = 0
        self.recent_errors = 0
        self.recent_429s = 0


# ================================================================================================
# EXTRA√á√ÉO DE SITEMAPS
# ================================================================================================
async def buscar_sitemaps(base_url: str, rate_limiter: AdaptiveRateLimiter, progress_callback: Optional[Callable] = None) -> List[str]:
    """
    Busca URLs de produtos nos sitemaps.
    Processa sitemap index recursivamente.
    """
    domain = urlparse(base_url).netloc
    urls_produto = set()
    sitemaps_para_processar = []
    
    # URLs de sitemap para testar
    sitemap_iniciais = [
        urljoin(base_url, "/sitemap.xml"),
        urljoin(base_url, "/sitemap_index.xml"),
        urljoin(base_url, "/sitemap-products.xml"),
        urljoin(base_url, "/product-sitemap.xml"),
        urljoin(base_url, "/robots.txt"),  # Pode ter refer√™ncia a sitemaps
    ]
    
    async with httpx.AsyncClient(timeout=20.0, follow_redirects=True) as client:
        # Fase 1: Buscar sitemaps iniciais
        for sitemap_url in sitemap_iniciais:
            await rate_limiter.acquire()
            
            try:
                if progress_callback:
                    progress_callback(len(urls_produto), 0, sitemap_url, "coletando")
                
                response = await client.get(sitemap_url)
                
                if response.status_code == 200:
                    rate_limiter.report_success()
                    content = response.text
                    
                    # Se √© robots.txt, procura por sitemaps
                    if sitemap_url.endswith('robots.txt'):
                        sitemap_refs = re.findall(r'Sitemap:\s*(.+)', content, re.IGNORECASE)
                        sitemaps_para_processar.extend([ref.strip() for ref in sitemap_refs])
                    else:
                        # Parseia XML do sitemap
                        urls, sitemaps = extrair_urls_do_sitemap(content, domain)
                        urls_produto.update(urls)
                        sitemaps_para_processar.extend(sitemaps)
                else:
                    rate_limiter.report_error()
            
            except Exception:
                rate_limiter.report_error()
        
        # Fase 2: Processar sitemaps encontrados (sitemap index)
        for sitemap_url in sitemaps_para_processar:
            await rate_limiter.acquire()
            
            try:
                if progress_callback:
                    progress_callback(len(urls_produto), 0, f"Sub-sitemap: {sitemap_url}", "coletando")
                
                response = await client.get(sitemap_url)
                
                if response.status_code == 200:
                    rate_limiter.report_success()
                    urls, _ = extrair_urls_do_sitemap(response.text, domain)
                    urls_produto.update(urls)
                else:
                    rate_limiter.report_error()
            
            except Exception:
                rate_limiter.report_error()
    
    return list(urls_produto)


def extrair_urls_do_sitemap(xml_content: str, domain: str) -> tuple[set, set]:
    """
    Extrai URLs de um sitemap XML.
    Retorna: (urls_de_produtos, urls_de_sitemaps)
    """
    urls = set()
    sitemaps = set()
    
    try:
        # Remove namespaces para facilitar parsing
        xml_clean = re.sub(r' xmlns[^>]*', '', xml_content)
        root = ET.fromstring(xml_clean)
        
        # Verifica se √© sitemap index (<sitemapindex>)
        if 'sitemapindex' in root.tag.lower():
            # √â um index de sitemaps - procura <sitemap><loc>
            for sitemap_elem in root.findall('.//sitemap'):
                loc = sitemap_elem.find('loc')
                if loc is not None and loc.text:
                    url = loc.text.strip()
                    if domain in url:
                        sitemaps.add(url)
        else:
            # √â sitemap normal - procura <url><loc>
            for url_elem in root.findall('.//url'):
                loc = url_elem.find('loc')
                if loc is not None and loc.text:
                    url = loc.text.strip()
                    if domain in url:
                        urls.add(url)
    
    except:
        # Fallback: regex
        urls_regex = re.findall(r'<loc>(https?://[^<]+)</loc>', xml_content)
        for u in urls_regex:
            if domain in u:
                # Se cont√©m 'sitemap' no nome, √© sitemap
                if 'sitemap' in u.lower() and u.endswith('.xml'):
                    sitemaps.add(u)
                else:
                    urls.add(u)
    
    return urls, sitemaps


# ================================================================================================
# APRENDIZADO DE PADR√ïES
# ================================================================================================
def aprender_padrao_urls(urls_amostra: List[str], max_amostra: int = 100) -> Optional[re.Pattern]:
    """
    Aprende padr√£o de URLs de produto analisando uma amostra.
    Retorna regex para filtrar produtos rapidamente.
    """
    if not urls_amostra:
        return None
    
    # Pega amostra maior e mais distribu√≠da
    # Ignora as primeiras 20 URLs (geralmente s√£o institucionais)
    urls_skip = urls_amostra[20:] if len(urls_amostra) > 20 else urls_amostra
    amostra = urls_skip[:max_amostra]
    
    # Analisa padr√µes comuns (com threshold din√¢mico)
    padroes_comuns = [
        # Padr√µes tradicionais de e-commerce
        (r'/produtos?/[^/]+-\d+/?$', 'Gigabarato/WordPress: /produtos/nome-123/', 0.25),
        (r'/p(roduto)?/[^/]+/\d+', 'Magento/VTEX: /produto/nome/123 ou /p/nome/123', 0.5),
        (r'/[^/]+-p-\d+', 'VTEX: /nome-do-produto-p-123', 0.5),
        (r'/produto/[^/]+\.html', 'PrestaShop: /produto/nome.html', 0.5),
        (r'/[^/]+/p/\d+', 'VTEX: /categoria/p/123', 0.5),
        (r'\.com\.br/[^/]+-\d+/', 'WordPress: .com.br/produto-123/', 0.5),
        
        # Padr√µes para sites com estrutura profunda (MatConcasa, similares)
        # Aceita URLs com exatamente 4 segmentos: /cat1/cat2/cat3/produto-final
        (r'^https?://[^/]+/[^/]+/[^/]+/[^/]+/?$', 'Categoria n√≠vel 3 (produtos finais)', 0.15),
        # Aceita URLs com 5+ segmentos (muito espec√≠ficas)
        (r'^https?://[^/]+/[^/]+/[^/]+/[^/]+/[^/]+/?', 'Categoria profunda 4+ (produtos)', 0.10),
    ]
    
    # Testa qual padr√£o melhor se aplica
    melhor_padrao = None
    melhor_score = 0
    melhor_nome = ''
    
    for padrao_str, nome, threshold in padroes_comuns:
        padrao = re.compile(padrao_str)
        matches = sum(1 for url in amostra if padrao.search(url))
        score = matches / len(amostra)
        
        if score >= threshold and score > melhor_score:
            melhor_score = score
            melhor_padrao = padrao
            melhor_nome = nome
    
    # Se encontrou padr√£o acima do threshold, retorna
    if melhor_padrao:
        return melhor_padrao
    
    return None


# ================================================================================================
# VALIDA√á√ÉO DE PRODUTOS
# ================================================================================================
async def validar_produto(url: str, client: httpx.AsyncClient, rate_limiter: AdaptiveRateLimiter) -> bool:
    """
    Valida se URL √© realmente um produto fazendo requisi√ß√£o HTTP.
    Retorna True se for produto v√°lido.
    """
    await rate_limiter.acquire()
    
    try:
        response = await client.get(url, timeout=10.0)
        
        if response.status_code == 429:
            rate_limiter.report_429()
            await asyncio.sleep(2)  # Reduzido de 5s para 2s
            return False
        
        if response.status_code != 200:
            rate_limiter.report_error()
            return False
        
        rate_limiter.report_success()
        
        # Verifica se tem indicadores de produto na p√°gina
        html = response.text.lower()
        
        indicadores_produto = [
            'application/ld+json',  # JSON-LD
            'og:type',              # Open Graph
            'product',              # Palavra produto
            'price',                # Pre√ßo
            'add to cart',          # Bot√£o comprar
            'buy now',
            'comprar',
        ]
        
        score = sum(1 for ind in indicadores_produto if ind in html)
        
        return score >= 2  # Pelo menos 2 indicadores
    
    except:
        rate_limiter.report_error()
        return False


# ================================================================================================
# VALIDA√á√ÉO ADAPTATIVA INTELIGENTE
# ================================================================================================
async def validacao_adaptativa(
    urls: List[str],
    rate_limiter: AdaptiveRateLimiter,
    show_message: Callable,
    progress_callback: Optional[Callable],
    max_produtos: Optional[int] = None
) -> List[str]:
    """
    Valida√ß√£o adaptativa com DETEC√á√ÉO DE PADR√ÉO EARLY-STOP:
    - Valida 10-20 URLs
    - Tenta detectar padr√£o
    - Se encontrar padr√£o: PARA e usa padr√£o no resto!
    - Se n√£o encontrar: continua valida√ß√£o adaptativa
    """
    total_urls = len(urls)
    
    # Heur√≠stica: Prioriza URLs por profundidade
    # N√≠vel 3 (4 barras): /cat1/cat2/produto - PRIORIDADE M√ÅXIMA (MatConcasa, etc)
    # N√≠vel 4+ (5+ barras): /cat1/cat2/cat3/produto - PRIORIDADE ALTA
    # N√≠vel 2 (3 barras): /categoria/produto - PRIORIDADE M√âDIA
    urls_nivel3 = [url for url in urls if url.count('/') == 4]  # /a/b/c
    urls_nivel4plus = [url for url in urls if url.count('/') >= 5]  # /a/b/c/d/e
    urls_nivel2 = [url for url in urls if url.count('/') == 3]  # /a/b
    urls_resto = [url for url in urls if url.count('/') < 3 or url.count('/') > 5]
    
    # Reordena: n√≠vel 3 primeiro, depois 4+, depois 2, depois resto
    urls_reordenadas = urls_nivel3 + urls_nivel4plus + urls_nivel2 + urls_resto
    
    if len(urls_nivel3) >= 20:
        show_message(f"üéØ Detectou {len(urls_nivel3)} URLs n√≠vel 3 (formato: /cat/sub/produto). Priorizando...")
        urls = urls_reordenadas
    elif len(urls_nivel4plus) >= 20:
        show_message(f"üéØ Detectou {len(urls_nivel4plus)} URLs profundas (4+ n√≠veis). Priorizando...")
        urls = urls_reordenadas
    
    # FASE 1: Valida APENAS 20 URLs e tenta detectar padr√£o
    amostra_minima = 20
    show_message(f"üîç Validando {amostra_minima} URLs e procurando padr√£o...")
    
    urls_validas = []
    
    async with httpx.AsyncClient(
        headers={'User-Agent': 'Mozilla/5.0'},
        timeout=10.0,
        follow_redirects=True
    ) as client:
        
        # Valida primeira amostra m√≠nima
        for i, url in enumerate(urls[:amostra_minima]):
            if progress_callback:
                progress_callback(i + 1, amostra_minima, "", "validando")
            
            if await validar_produto(url, client, rate_limiter):
                urls_validas.append(url)
        
        # TENTA DETECTAR PADR√ÉO logo ap√≥s 20 valida√ß√µes
        if len(urls_validas) >= 10:  # Precisa de pelo menos 10 v√°lidas
            show_message(f"üß† Tentando detectar padr√£o com {len(urls_validas)} URLs v√°lidas...")
            padrao = aprender_padrao_urls(urls_validas, max_amostra=len(urls_validas))
            
            if padrao:
                # üéâ ACHOU PADR√ÉO! Para de validar e usa padr√£o no resto!
                show_message(f"‚úÖ PADR√ÉO DETECTADO: {padrao.pattern}")
                show_message(f"üöÄ Aplicando padr√£o no resto (SEM valida√ß√£o HTTP)!")
                
                # Aplica padr√£o em TODAS as URLs restantes (sem HTTP)
                urls_com_padrao = [url for url in urls[amostra_minima:] if padrao.search(url)]
                urls_validas.extend(urls_com_padrao)
                
                if max_produtos and len(urls_validas) > max_produtos:
                    urls_validas = urls_validas[:max_produtos]
                
                show_message(f"‚úÖ Total: {len(urls_validas)} produtos (padr√£o aplicado em {len(urls_com_padrao)})")
                return urls_validas
        
        # Se n√£o achou padr√£o, continua valida√ß√£o adaptativa
        show_message(f"‚ö†Ô∏è Padr√£o n√£o detectado. Continuando valida√ß√£o adaptativa...")
        
        # FASE 2: Valida mais 30 URLs (total 50)
        amostra_extra = 30
        for i, url in enumerate(urls[amostra_minima:amostra_minima + amostra_extra]):
            if progress_callback:
                progress_callback(amostra_minima + i + 1, amostra_minima + amostra_extra, "", "validando")
            
            if await validar_produto(url, client, rate_limiter):
                urls_validas.append(url)
        
        taxa_sucesso = len(urls_validas) / (amostra_minima + amostra_extra)
        show_message(f"üìä Taxa de sucesso: {taxa_sucesso*100:.1f}% ({len(urls_validas)}/{amostra_minima + amostra_extra})")
        
        # Decis√£o inteligente
        if taxa_sucesso >= 0.80:
            # Alta taxa = assume que resto √© v√°lido
            show_message(f"‚úÖ Alta taxa! Assumindo resto como v√°lido")
            urls_validas.extend(urls[amostra_minima + amostra_extra:max_produtos] if max_produtos else urls[amostra_minima + amostra_extra:])
        
        elif taxa_sucesso >= 0.50:
            # Taxa m√©dia = valida mais 100 URLs
            show_message(f"‚ö†Ô∏è Taxa m√©dia. Validando mais 100 URLs...")
            amostra_adicional = 100
            
            for i, url in enumerate(urls[amostra_minima + amostra_extra:amostra_minima + amostra_extra + amostra_adicional]):
                if progress_callback:
                    progress_callback(amostra_minima + amostra_extra + i + 1, amostra_minima + amostra_extra + amostra_adicional, "", "validando")
                
                if await validar_produto(url, client, rate_limiter):
                    urls_validas.append(url)
            
            # Recalcula taxa
            taxa_final = len(urls_validas) / (amostra_minima + amostra_extra + amostra_adicional)
            show_message(f"üìä Taxa final: {taxa_final*100:.1f}%")
            
            if taxa_final >= 0.70:
                show_message(f"‚úÖ Taxa aceit√°vel. Assumindo resto como v√°lido")
                urls_validas.extend(urls[amostra_minima + amostra_extra + amostra_adicional:max_produtos] if max_produtos else urls[amostra_minima + amostra_extra + amostra_adicional:])
        
        else:
            # Taxa baixa < 50% = valida at√© 500
            show_message(f"‚ùå Taxa baixa! Validando at√© 500 URLs...")
            limite = min(500, len(urls)) if not max_produtos else min(max_produtos, len(urls))
            
            for i, url in enumerate(urls[amostra_minima + amostra_extra:limite]):
                if progress_callback:
                    progress_callback(amostra_minima + amostra_extra + i + 1, limite, "", "validando")
                
                if await validar_produto(url, client, rate_limiter):
                    urls_validas.append(url)
    
    show_message(f"‚úÖ Valida√ß√£o conclu√≠da: {len(urls_validas)} produtos de {total_urls} URLs")
    return urls_validas


# ================================================================================================
# FUN√á√ÉO PRINCIPAL
# ================================================================================================
async def extrair_produtos_async(
    base_url: str,
    show_message: Callable,
    max_produtos: Optional[int] = None,
    progress_callback: Optional[Callable] = None
) -> List[Dict]:
    """
    Extrai produtos usando arquitetura Crawlee.
    
    Args:
        base_url: URL base do site
        show_message: Fun√ß√£o para exibir mensagens
        max_produtos: Limite de produtos (None = sem limite)
        progress_callback: Callback de progresso
    
    Returns:
        Lista de dicion√°rios com {nome, url}
    """
    show_message("üöÄ Iniciando extra√ß√£o Crawlee-style...")
    
    # Rate limiter adaptativo - AUMENTADO para 300 RPM (valida√ß√£o r√°pida)
    rate_limiter = AdaptiveRateLimiter(requests_per_minute=300, autoscale=True)
    
    # 1. Busca sitemaps
    show_message("üìã Buscando sitemaps...")
    urls_sitemap = await buscar_sitemaps(base_url, rate_limiter, progress_callback)
    
    if not urls_sitemap:
        show_message("‚ö†Ô∏è Nenhum sitemap encontrado")
        return []
    
    show_message(f"‚úÖ Encontrou {len(urls_sitemap)} URLs no sitemap")
    
    # 2. Aprende padr√£o de URLs
    if progress_callback:
        progress_callback(0, min(100, len(urls_sitemap)), "", "fase_aprendizado")
    
    show_message("üß† Aprendendo padr√µes de URLs...")
    padrao = aprender_padrao_urls(urls_sitemap, max_amostra=100)
    
    if padrao:
        show_message(f"‚úÖ Padr√£o identificado: {padrao.pattern}")
        
        # Filtra URLs usando padr√£o (sem HTTP)
        urls_filtradas = []
        for i, url in enumerate(urls_sitemap):
            if progress_callback:
                progress_callback(i + 1, len(urls_sitemap), "", "aplicando_padrao")
            
            if padrao.search(url):
                urls_filtradas.append(url)
                
                if max_produtos and len(urls_filtradas) >= max_produtos:
                    break
        
        show_message(f"‚úÖ Filtrou {len(urls_filtradas)} produtos usando padr√£o")
    else:
        show_message("‚ö†Ô∏è Padr√£o n√£o identificado, validando amostra adaptativa...")
        # Valida√ß√£o adaptativa inteligente
        urls_filtradas = await validacao_adaptativa(
            urls_sitemap, 
            rate_limiter, 
            show_message, 
            progress_callback,
            max_produtos
        )
    
    # 3. Valida√ß√£o (se necess√°rio)
    produtos = []
    
    if padrao and len(urls_filtradas) >= 10:
        # Padr√£o confi√°vel, n√£o precisa validar
        show_message(f"‚úÖ Usando padr√£o confi√°vel, sem valida√ß√£o HTTP")
        
        for url in urls_filtradas:
            nome = url.split('/')[-2].replace('-', ' ').title()
            if not nome or len(nome) < 3:
                nome = url.split('/')[-1].replace('-', ' ').title()
            
            produtos.append({
                'nome': nome,
                'url': url
            })
    else:
        # Precisa validar com HTTP
        show_message(f"üîç Validando {len(urls_filtradas)} URLs com HTTP...")
        
        async with httpx.AsyncClient(follow_redirects=True) as client:
            for i, url in enumerate(urls_filtradas):
                if progress_callback:
                    progress_callback(i + 1, len(urls_filtradas), "", "validando")
                
                if await validar_produto(url, client, rate_limiter):
                    # Extrai nome da URL
                    nome = url.split('/')[-2].replace('-', ' ').title()
                    if not nome or len(nome) < 3:
                        nome = url.split('/')[-1].replace('-', ' ').title()
                    
                    produtos.append({
                        'nome': nome,
                        'url': url
                    })
                    
                    if progress_callback:
                        progress_callback(len(produtos), 0, "", "produto_validado")
                    
                    if max_produtos and len(produtos) >= max_produtos:
                        break
    
    show_message(f"‚úÖ Extra√ß√£o conclu√≠da: {len(produtos)} produtos")
    
    return produtos


def extrair_produtos_rapido(
    base_url: str,
    show_message: Callable,
    max_produtos: Optional[int] = None,
    progress_callback: Optional[Callable] = None
) -> List[Dict]:
    """
    Wrapper s√≠ncrono para extrair_produtos_async.
    Compat√≠vel com a interface do appv4.py.
    """
    return asyncio.run(extrair_produtos_async(
        base_url, 
        show_message, 
        max_produtos, 
        progress_callback
    ))


# ================================================================================================
# TESTE
# ================================================================================================
if __name__ == "__main__":
    def dummy_message(msg):
        print(f"[INFO] {msg}")
    
    def dummy_progress(atual, total, info, tipo):
        if tipo == "coletando":
            print(f"[SITEMAP] {info}")
        elif tipo == "aplicando_padrao":
            print(f"[FILTRO] {atual}/{total}")
        elif tipo == "validando":
            print(f"[VALIDA√á√ÉO] {atual}/{total}")
        elif tipo == "produto_validado":
            print(f"[PRODUTOS] {atual}")
    
    # Teste com Bella Cotton
    produtos = extrair_produtos_rapido(
        "https://www.bellacotton.com.br",
        dummy_message,
        max_produtos=20,
        progress_callback=dummy_progress
    )
    
    print(f"\n‚úÖ {len(produtos)} produtos extra√≠dos:")
    for p in produtos[:5]:
        print(f"  - {p['nome']}")
