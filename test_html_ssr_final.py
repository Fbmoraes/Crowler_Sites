"""
üöÄ VERS√ÉO FINAL OTIMIZADA - Baseline + 3 Otimiza√ß√µes Ben√©ficas
- lxml parser (20-30% mais r√°pido)
- Regex pr√©-compiladas (evita recompila√ß√£o)
- Retry-After + jitter inteligente (melhor tratamento de 429)
"""

import asyncio
import httpx
import time
import json
import random
from bs4 import BeautifulSoup
import re

# User-Agents para rota√ß√£o
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
]

# ‚úÖ OTIMIZA√á√ÉO 1: Regex pr√©-compiladas (evita recompila√ß√£o a cada produto)
PRECO_RE = re.compile(r'R\$\s*(?:<!--.*?-->)?\s*([\d.,]+)', re.DOTALL)
SKU_RE = re.compile(r'-(\d+)$')
MARCA_RE = re.compile(r'marca[:\s]*([A-Z][A-Za-z]+)', re.IGNORECASE)


class TokenBucket:
    """Rate limiter simples"""
    def __init__(self, rate: float):
        self.rate = rate
        self.tokens = rate
        self.last_update = time.time()
        self.lock = asyncio.Lock()
    
    async def acquire(self):
        async with self.lock:
            now = time.time()
            elapsed = now - self.last_update
            self.tokens = min(self.rate, self.tokens + elapsed * self.rate)
            self.last_update = now
            
            if self.tokens < 1:
                wait_time = (1 - self.tokens) / self.rate
                await asyncio.sleep(wait_time)
                self.tokens = 0
            else:
                self.tokens -= 1


def parse_retry_after(header_value):
    """‚úÖ OTIMIZA√á√ÉO 3: Parse Retry-After header (segundos ou data HTTP)"""
    if not header_value:
        return None
    try:
        return int(header_value)
    except ValueError:
        return None


async def extrair_produto_via_html(
    client: httpx.AsyncClient, 
    url: str, 
    rate_limiter: TokenBucket,
    max_retries: int = 3
) -> dict:
    """Extrai produto direto do HTML com retry e rate limiting"""
    
    for tentativa in range(max_retries):
        inicio = time.time()
        
        try:
            # Rate limiting
            await rate_limiter.acquire()
            
            # Headers COMPLETOS (n√£o simplificar - servidor pode responder diferente!)
            headers = {
                "User-Agent": random.choice(USER_AGENTS),
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "pt-BR,pt;q=0.9,en-US;q=0.8,en;q=0.7",
                "Referer": "https://www.matconcasa.com.br/",
                "DNT": "1",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "same-origin",
                "Cache-Control": "max-age=0",
            }
            
            response = await client.get(url, headers=headers, timeout=10, follow_redirects=True)
            
            # ‚úÖ OTIMIZA√á√ÉO 3: Retry-After + jitter inteligente
            if response.status_code == 429:
                if tentativa < max_retries - 1:
                    retry_delay = parse_retry_after(response.headers.get("Retry-After"))
                    if retry_delay is None:
                        retry_delay = min(8 * (2 ** tentativa), 60)
                    # Jitter: +/- 50% (evita thundering herd)
                    retry_delay *= random.uniform(0.5, 1.5)
                    await asyncio.sleep(retry_delay)
                    continue
                else:
                    return {"erro": "HTTP 429 - Max retries atingido", "status": 429, "url": url}
            
            if response.status_code != 200:
                return {"erro": f"HTTP {response.status_code}", "status": response.status_code, "url": url}
            
            html = response.text
            
            # Verificar HTML vazio
            if not html or len(html) < 1000:
                if tentativa < max_retries - 1:
                    await asyncio.sleep(2 ** tentativa)
                    continue
                return {"erro": f"HTML vazio ({len(html)} bytes)", "url": url}
            
            # ‚úÖ OTIMIZA√á√ÉO 2: Parser lxml (20-30% mais r√°pido que html.parser)
            try:
                soup = BeautifulSoup(html, 'lxml')
            except:
                soup = BeautifulSoup(html, 'html.parser')  # Fallback
            
            produto = {
                'nome': None,
                'preco': None,
                'preco_original': None,
                'imagens': [],
                'marca': 'N√£o informado',
                'sku': None,
                'disponivel': True,
            }
            
            # 1. NOME - O segundo H1 √© sempre o nome do produto
            nome = None
            h1_tags = soup.find_all('h1')
            
            if len(h1_tags) >= 2:
                nome = h1_tags[1].get_text(strip=True)
            elif h1_tags:
                nome = h1_tags[0].get_text(strip=True)
            
            # Fallback: usar title da p√°gina
            if not nome or len(nome) < 10:
                title = soup.find('title')
                if title:
                    parts = title.get_text().split('|')
                    if len(parts) >= 2:
                        nome = parts[-1].strip()
            
            produto['nome'] = nome
            
            # 2. PRE√áOS - ‚úÖ Usar regex pr√©-compilada
            precos_encontrados = PRECO_RE.findall(html)  # Buscar no HTML original
            
            if precos_encontrados:
                precos_num = []
                for p in precos_encontrados:
                    try:
                        valor = float(p.replace('.', '').replace(',', '.'))
                        if valor > 10:
                            precos_num.append((valor, p))
                    except:
                        pass
                
                if len(precos_num) >= 2:
                    precos_num.sort(key=lambda x: x[0], reverse=True)
                    produto['preco_original'] = precos_num[0][1]
                    produto['preco'] = precos_num[1][1]
                elif precos_num:
                    produto['preco'] = precos_num[0][1]
            
            # 3. IMAGENS
            imgs = soup.find_all('img')
            imagens_unicas = []
            seen = set()
            for img in imgs:
                src = img.get('src', '')
                if src and ('produto' in src.lower() or 'product' in src.lower()):
                    img_base = src.split('?')[0]
                    if img_base not in seen:
                        seen.add(img_base)
                        imagens_unicas.append(src)
            
            produto['imagens'] = imagens_unicas[:5]
            
            # 4. DISPONIBILIDADE
            texto_completo = soup.get_text()
            produto['disponivel'] = 'indispon√≠vel' not in texto_completo.lower()
            
            # 5. SKU - ‚úÖ Usar regex pr√©-compilada
            sku_match = SKU_RE.search(url)
            if sku_match:
                produto['sku'] = sku_match.group(1)
            
            # 6. MARCA - ‚úÖ Usar regex pr√©-compilada
            marca_match = MARCA_RE.search(texto_completo)
            if marca_match:
                produto['marca'] = marca_match.group(1)
            
            produto['metodo'] = 'HTML-SSR-Optimized'
            produto['tempo'] = time.time() - inicio
            produto['tentativas'] = tentativa + 1
            produto['url'] = url
            produto['bytes'] = len(html)
            
            return produto
            
        except httpx.TimeoutException:
            if tentativa < max_retries - 1:
                await asyncio.sleep(2 ** tentativa)
                continue
            return {"erro": "Timeout", "url": url}
        except Exception as e:
            if tentativa < max_retries - 1:
                await asyncio.sleep(1)
                continue
            return {"erro": str(e), "url": url}
    
    return {"erro": "Max retries atingido", "url": url}


async def testar_50_urls():
    """Teste de carga com 50 URLs"""
    
    # Carregar URLs do arquivo
    with open('urls_matcon_100.txt', 'r', encoding='utf-8') as f:
        urls = [line.strip() for line in f if line.strip()][:50]
    
    print("=" * 100)
    print("üöÄ TESTE FINAL - Baseline + 3 Otimiza√ß√µes Ben√©ficas")
    print("=" * 100)
    print(f"üì¶ URLs carregadas: {len(urls)}")
    print(f"‚ö° Rate limit: 4 requests/second")
    print(f"üîÑ Concorr√™ncia: 3 URLs simult√¢neas")
    print(f"‚è±Ô∏è  Timeout: 10s por request")
    print()
    print("‚úÖ OTIMIZA√á√ïES ATIVAS:")
    print("   1. Parser lxml (20-30% mais r√°pido)")
    print("   2. Regex pr√©-compiladas (evita recompila√ß√£o)")
    print("   3. Retry-After + jitter inteligente")
    print("=" * 100)
    print()
    
    rate_limiter = TokenBucket(rate=4.0)
    resultados = []
    erros = 0
    erros_429 = 0
    total_bytes = 0
    
    concorrencia = 3
    
    async with httpx.AsyncClient() as client:
        inicio_total = time.time()
        
        for lote_idx in range(0, len(urls), concorrencia):
            lote = urls[lote_idx:lote_idx + concorrencia]
            
            print(f"[Lote {lote_idx//concorrencia + 1}/{(len(urls)-1)//concorrencia + 1}] Processando {len(lote)} URLs...")
            
            tasks = [extrair_produto_via_html(client, url, rate_limiter) for url in lote]
            produtos = await asyncio.gather(*tasks)
            
            for i, (url, produto) in enumerate(zip(lote, produtos), 1):
                idx_global = lote_idx + i
                
                if 'erro' in produto:
                    print(f"  [{idx_global:2d}/50] ‚ùå {produto['erro'][:50]}")
                    erros += 1
                    if produto.get('status') == 429:
                        erros_429 += 1
                else:
                    nome = produto.get('nome', 'SEM NOME')[:45]
                    preco = produto.get('preco', 'N/A')
                    tempo = produto.get('tempo', 0)
                    bytes_val = produto.get('bytes', 0) / 1024
                    total_bytes += produto.get('bytes', 0)
                    print(f"  [{idx_global:2d}/50] ‚úÖ {nome:45s} | R$ {preco:>10s} | {tempo:.2f}s | {bytes_val:.1f}KB")
                
                resultados.append(produto)
            
            print()
        
        tempo_total = time.time() - inicio_total
        sucesso = len(urls) - erros
        tempo_medio = tempo_total / len(urls)
        velocidade = len(urls) / tempo_total
        
        print("=" * 100)
        print("üìä RESULTADOS FINAIS")
        print("=" * 100)
        print(f"‚úÖ Sucesso: {sucesso}/{len(urls)} ({sucesso/len(urls)*100:.1f}%)")
        print(f"‚ùå Erros: {erros}")
        if erros_429 > 0:
            print(f"‚ö†Ô∏è  Erros 429 (Rate Limit): {erros_429}")
        print(f"‚è±Ô∏è  Tempo total: {tempo_total:.2f}s")
        print(f"‚ö° Tempo m√©dio: {tempo_medio:.3f}s por produto")
        print(f"üöÄ Velocidade: {velocidade:.2f} produtos/segundo")
        if sucesso > 0:
            print(f"üì¶ Total baixado: {total_bytes/1024/1024:.2f} MB (m√©dia: {total_bytes/sucesso/1024:.1f} KB/produto)")
        
        # Compara√ß√£o
        print(f"\nüìà COMPARA√á√ÉO COM BASELINE:")
        print(f"   Baseline: 1.006s/produto (50.32s total)")
        print(f"   Otimizada: {tempo_medio:.3f}s/produto ({tempo_total:.2f}s total)")
        ganho = ((1.006 - tempo_medio) / 1.006 * 100)
        if ganho > 0:
            print(f"   üéâ Ganho: {ganho:.1f}% MAIS R√ÅPIDO!")
        else:
            print(f"   üìä Diferen√ßa: {abs(ganho):.1f}% mais lento")
        
        estimativa_800 = tempo_medio * 800
        print(f"\n   Estimativa 800 produtos:")
        print(f"   Baseline: 13.4 minutos")
        print(f"   Otimizada: {estimativa_800/60:.1f} minutos")
        print("=" * 100)
        
        # Salvar resultados
        arquivo_resultado = 'resultados_html_ssr_final.json'
        with open(arquivo_resultado, 'w', encoding='utf-8') as f:
            json.dump(resultados, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ Resultados salvos em: {arquivo_resultado}")
        
        # Mostrar exemplos
        print("\nüìã EXEMPLOS:\n")
        produtos_sucesso = [p for p in resultados if 'erro' not in p][:3]
        for i, p in enumerate(produtos_sucesso, 1):
            print(f"{i}. {p.get('nome', 'N/A')}")
            print(f"   Pre√ßo: R$ {p.get('preco', 'N/A')}")
            if p.get('preco_original'):
                print(f"   De: R$ {p.get('preco_original')}")
            print(f"   SKU: {p.get('sku', 'N/A')}")
            print(f"   Tempo: {p.get('tempo', 0):.3f}s")
            print(f"   Bytes: {p.get('bytes', 0)/1024:.1f}KB")
            print()


if __name__ == "__main__":
    asyncio.run(testar_50_urls())
